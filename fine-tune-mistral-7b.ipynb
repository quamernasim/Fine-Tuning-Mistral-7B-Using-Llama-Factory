{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clone Llama-Factory and Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/hiyouga/LLaMA-Factory.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/quamer/Fine-Tuning-Mistral-7B-Using-Llama-Factory/LLaMA-Factory\n"
     ]
    }
   ],
   "source": [
    "%cd LLaMA-Factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Llama-Factory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will run on localhost only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 python src/train_web.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to get an external URL, you'll need to set share=True in train_web.py like shown in the image below\n",
    "\n",
    "<img src=\"assets/train_web.png\" alt=\"share=True\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "dataset = {\n",
    "    'dockerNLCommands': {\n",
    "        \"hf_hub_url\": \"MattCoddity/dockerNLcommands\",\n",
    "        \"columns\": {\n",
    "            \"prompt\": \"instruction\",\n",
    "            \"query\": \"input\",\n",
    "            \"response\": \"output\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "template = {\n",
    "    _register_template(\n",
    "        name=\"mistral-instruction-v02\",\n",
    "        format_user=StringFormatter(slots=[\"[INST] {{content}} [/INST]\"]),\n",
    "        format_system=StringFormatter(slots=[{\"bos_token\"}, \"{{content}}\"]),\n",
    "        force_system=True,\n",
    ")\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://0.0.0.0:7860\n",
      "Running on public URL: https://77d2ebff7d57c17836.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n",
      "03/13/2024 10:49:34 - WARNING - llmtuner.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.\n",
      "03/13/2024 10:49:34 - INFO - llmtuner.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.bfloat16\n",
      "[INFO|tokenization_utils_base.py:2046] 2024-03-13 10:49:34,751 >> loading file tokenizer.model from cache at /home/ubuntu/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/cf47bb3e18fe41a5351bc36eef76e9c900847c89/tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:2046] 2024-03-13 10:49:34,751 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2046] 2024-03-13 10:49:34,751 >> loading file special_tokens_map.json from cache at /home/ubuntu/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/cf47bb3e18fe41a5351bc36eef76e9c900847c89/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2046] 2024-03-13 10:49:34,751 >> loading file tokenizer_config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/cf47bb3e18fe41a5351bc36eef76e9c900847c89/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2046] 2024-03-13 10:49:34,751 >> loading file tokenizer.json from cache at /home/ubuntu/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/cf47bb3e18fe41a5351bc36eef76e9c900847c89/tokenizer.json\n",
      "03/13/2024 10:49:34 - INFO - llmtuner.data.template - Add pad token: </s>\n",
      "03/13/2024 10:49:34 - INFO - llmtuner.data.loader - Loading dataset MattCoddity/dockerNLcommands...\n",
      "input_ids:\n",
      "[1, 733, 16289, 28793, 17824, 456, 12271, 297, 281, 14295, 3445, 13, 28777, 495, 528, 264, 1274, 302, 25399, 369, 506, 272, 500, 28726, 2794, 28718, 3469, 390, 652, 14014, 271, 28723, 733, 28748, 16289, 28793, 281, 14295, 12384, 1939, 4650, 464, 834, 374, 271, 28746, 437, 2794, 28718, 28742, 2]\n",
      "inputs:\n",
      "<s> [INST] translate this sentence in docker command\n",
      "Give me a list of containers that have the Ubuntu image as their ancestor. [/INST] docker ps --filter 'ancestor=ubuntu'</s>\n",
      "label_ids:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 281, 14295, 12384, 1939, 4650, 464, 834, 374, 271, 28746, 437, 2794, 28718, 28742, 2]\n",
      "labels:\n",
      " docker ps --filter 'ancestor=ubuntu'</s>\n",
      "[INFO|configuration_utils.py:728] 2024-03-13 10:49:35,832 >> loading configuration file config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/cf47bb3e18fe41a5351bc36eef76e9c900847c89/config.json\n",
      "[INFO|configuration_utils.py:791] 2024-03-13 10:49:35,840 >> Model config MistralConfig {\n",
      "  \"_name_or_path\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.38.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "03/13/2024 10:49:35 - WARNING - llmtuner.model.patcher - FlashAttention2 is not installed.\n",
      "03/13/2024 10:49:35 - INFO - llmtuner.model.patcher - Quantizing model to 4 bit.\n",
      "[INFO|modeling_utils.py:3257] 2024-03-13 10:49:36,286 >> loading weights file model.safetensors from cache at /home/ubuntu/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/cf47bb3e18fe41a5351bc36eef76e9c900847c89/model.safetensors.index.json\n",
      "[INFO|modeling_utils.py:1400] 2024-03-13 10:49:36,287 >> Instantiating MistralForCausalLM model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:845] 2024-03-13 10:49:36,287 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [00:02<00:00,  1.08it/s]\n",
      "[INFO|modeling_utils.py:3992] 2024-03-13 10:49:39,473 >> All model checkpoint weights were used when initializing MistralForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4000] 2024-03-13 10:49:39,473 >> All the weights of MistralForCausalLM were initialized from the model checkpoint at mistralai/Mistral-7B-Instruct-v0.2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MistralForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:800] 2024-03-13 10:49:39,747 >> loading configuration file generation_config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/cf47bb3e18fe41a5351bc36eef76e9c900847c89/generation_config.json\n",
      "[INFO|configuration_utils.py:845] 2024-03-13 10:49:39,748 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      "\n",
      "03/13/2024 10:49:39 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.\n",
      "03/13/2024 10:49:39 - INFO - llmtuner.model.adapter - Fine-tuning method: LoRA\n",
      "03/13/2024 10:49:39 - INFO - llmtuner.model.loader - trainable params: 3407872 || all params: 7245139968 || trainable%: 0.0470\n",
      "[INFO|trainer.py:601] 2024-03-13 10:49:40,016 >> Using auto half precision backend\n",
      "[INFO|trainer.py:1812] 2024-03-13 10:49:40,156 >> ***** Running training *****\n",
      "[INFO|trainer.py:1813] 2024-03-13 10:49:40,157 >>   Num examples = 2,173\n",
      "[INFO|trainer.py:1814] 2024-03-13 10:49:40,157 >>   Num Epochs = 10\n",
      "[INFO|trainer.py:1815] 2024-03-13 10:49:40,157 >>   Instantaneous batch size per device = 64\n",
      "[INFO|trainer.py:1818] 2024-03-13 10:49:40,157 >>   Total train batch size (w. parallel, distributed & accumulation) = 512\n",
      "[INFO|trainer.py:1819] 2024-03-13 10:49:40,157 >>   Gradient Accumulation steps = 8\n",
      "[INFO|trainer.py:1820] 2024-03-13 10:49:40,157 >>   Total optimization steps = 40\n",
      "[INFO|trainer.py:1821] 2024-03-13 10:49:40,158 >>   Number of trainable parameters = 3,407,872\n",
      "03/13/2024 10:52:38 - INFO - llmtuner.extras.callbacks - {'loss': 1.7795, 'learning_rate': 1.9239e-04, 'epoch': 1.18}\n",
      "{'loss': 1.7795, 'grad_norm': 4.0578837394714355, 'learning_rate': 0.0001923879532511287, 'epoch': 1.18}\n",
      "03/13/2024 10:55:42 - INFO - llmtuner.extras.callbacks - {'loss': 0.3294, 'learning_rate': 1.7071e-04, 'epoch': 2.35}\n",
      "{'loss': 0.3294, 'grad_norm': 1.268104910850525, 'learning_rate': 0.00017071067811865476, 'epoch': 2.35}\n",
      "03/13/2024 10:58:49 - INFO - llmtuner.extras.callbacks - {'loss': 0.1796, 'learning_rate': 1.3827e-04, 'epoch': 3.53}\n",
      "{'loss': 0.1796, 'grad_norm': 0.66749507188797, 'learning_rate': 0.000138268343236509, 'epoch': 3.53}\n",
      "03/13/2024 11:01:54 - INFO - llmtuner.extras.callbacks - {'loss': 0.0994, 'learning_rate': 1.0000e-04, 'epoch': 4.71}\n",
      "{'loss': 0.0994, 'grad_norm': 0.8153815269470215, 'learning_rate': 0.0001, 'epoch': 4.71}\n",
      "03/13/2024 11:05:00 - INFO - llmtuner.extras.callbacks - {'loss': 0.0648, 'learning_rate': 6.1732e-05, 'epoch': 5.88}\n",
      "{'loss': 0.0648, 'grad_norm': 0.5003596544265747, 'learning_rate': 6.173165676349103e-05, 'epoch': 5.88}\n",
      "03/13/2024 11:08:06 - INFO - llmtuner.extras.callbacks - {'loss': 0.0481, 'learning_rate': 2.9289e-05, 'epoch': 7.06}\n",
      "{'loss': 0.0481, 'grad_norm': 0.6051201820373535, 'learning_rate': 2.9289321881345254e-05, 'epoch': 7.06}\n",
      "03/13/2024 11:11:10 - INFO - llmtuner.extras.callbacks - {'loss': 0.0388, 'learning_rate': 7.6120e-06, 'epoch': 8.24}\n",
      "{'loss': 0.0388, 'grad_norm': 0.39773306250572205, 'learning_rate': 7.612046748871327e-06, 'epoch': 8.24}\n",
      "03/13/2024 11:14:16 - INFO - llmtuner.extras.callbacks - {'loss': 0.0355, 'learning_rate': 0.0000e+00, 'epoch': 9.41}\n",
      "{'loss': 0.0355, 'grad_norm': 0.3412567377090454, 'learning_rate': 0.0, 'epoch': 9.41}\n",
      "[INFO|trainer.py:2067] 2024-03-13 11:14:16,454 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "03/13/2024 11:14:16 - INFO - llmtuner.extras.callbacks - {'loss': 0.0000, 'learning_rate': 0.0000e+00, 'epoch': 9.41}\n",
      "{'train_runtime': 1476.2962, 'train_samples_per_second': 14.719, 'train_steps_per_second': 0.027, 'train_loss': 0.3219085168093443, 'epoch': 9.41}\n",
      "[INFO|trainer.py:3067] 2024-03-13 11:14:16,458 >> Saving model checkpoint to saves/Custom/lora/train_2024-03-13-10-46-24\n",
      "[INFO|configuration_utils.py:728] 2024-03-13 11:14:16,562 >> loading configuration file config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/cf47bb3e18fe41a5351bc36eef76e9c900847c89/config.json\n",
      "[INFO|configuration_utils.py:791] 2024-03-13 11:14:16,563 >> Model config MistralConfig {\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.38.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2459] 2024-03-13 11:14:16,701 >> tokenizer config file saved in saves/Custom/lora/train_2024-03-13-10-46-24/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2468] 2024-03-13 11:14:16,708 >> Special tokens file saved in saves/Custom/lora/train_2024-03-13-10-46-24/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =       9.41\n",
      "  train_loss               =     0.3219\n",
      "  train_runtime            = 0:24:36.29\n",
      "  train_samples_per_second =     14.719\n",
      "  train_steps_per_second   =      0.027\n",
      "[INFO|trainer.py:3376] 2024-03-13 11:14:16,764 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3378] 2024-03-13 11:14:16,764 >>   Num examples = 242\n",
      "[INFO|trainer.py:3381] 2024-03-13 11:14:16,764 >>   Batch size = 64\n",
      "03/13/2024 11:14:22 - INFO - llmtuner.extras.callbacks - {'loss': 0.0000, 'learning_rate': 0.0000e+00, 'epoch': 9.41}\n",
      "{'eval_loss': 0.02831052616238594, 'eval_runtime': 5.7017, 'eval_samples_per_second': 42.443, 'eval_steps_per_second': 0.702, 'epoch': 9.41}\n",
      "***** eval metrics *****\n",
      "  epoch                   =       9.41\n",
      "  eval_loss               =     0.0283\n",
      "  eval_runtime            = 0:00:05.70\n",
      "  eval_samples_per_second =     42.443\n",
      "  eval_steps_per_second   =      0.702\n",
      "[INFO|modelcard.py:450] 2024-03-13 11:14:22,491 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/gradio/queueing.py\", line 407, in call_prediction\n",
      "    output = await route_utils.call_process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/gradio/route_utils.py\", line 226, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/gradio/blocks.py\", line 1550, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/gradio/blocks.py\", line 1199, in call_function\n",
      "    prediction = await utils.async_iteration(iterator)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/gradio/utils.py\", line 519, in async_iteration\n",
      "    return await iterator.__anext__()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/gradio/utils.py\", line 512, in __anext__\n",
      "    return await anyio.to_thread.run_sync(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 2144, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 851, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/gradio/utils.py\", line 495, in run_sync_iterator_async\n",
      "    return next(iterator)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/gradio/utils.py\", line 649, in gen_wrapper\n",
      "    yield from f(*args, **kwargs)\n",
      "  File \"/home/ubuntu/quamer/Fine-Tuning-Mistral-7B-Using-Llama-Factory/LLaMA-Factory/src/llmtuner/webui/runner.py\", line 270, in preview_eval\n",
      "    yield from self._preview(data, do_train=False)\n",
      "  File \"/home/ubuntu/quamer/Fine-Tuning-Mistral-7B-Using-Llama-Factory/LLaMA-Factory/src/llmtuner/webui/runner.py\", line 245, in _preview\n",
      "    error = self._initialize(data, do_train, from_preview=True)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/Fine-Tuning-Mistral-7B-Using-Llama-Factory/LLaMA-Factory/src/llmtuner/webui/runner.py\", line 55, in _initialize\n",
      "    stage = TRAINING_STAGES[get(\"train.training_stage\")]\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/Fine-Tuning-Mistral-7B-Using-Llama-Factory/LLaMA-Factory/src/llmtuner/webui/runner.py\", line 52, in <lambda>\n",
      "    get = lambda name: data[self.manager.get_elem_by_name(name)]\n",
      "                       ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyError: dropdown\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/gradio/queueing.py\", line 407, in call_prediction\n",
      "    output = await route_utils.call_process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/gradio/route_utils.py\", line 226, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/gradio/blocks.py\", line 1550, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/gradio/blocks.py\", line 1199, in call_function\n",
      "    prediction = await utils.async_iteration(iterator)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/gradio/utils.py\", line 519, in async_iteration\n",
      "    return await iterator.__anext__()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/gradio/utils.py\", line 512, in __anext__\n",
      "    return await anyio.to_thread.run_sync(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 2144, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 851, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/gradio/utils.py\", line 495, in run_sync_iterator_async\n",
      "    return next(iterator)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/gradio/utils.py\", line 649, in gen_wrapper\n",
      "    yield from f(*args, **kwargs)\n",
      "  File \"/home/ubuntu/quamer/Fine-Tuning-Mistral-7B-Using-Llama-Factory/LLaMA-Factory/src/llmtuner/webui/runner.py\", line 270, in preview_eval\n",
      "    yield from self._preview(data, do_train=False)\n",
      "  File \"/home/ubuntu/quamer/Fine-Tuning-Mistral-7B-Using-Llama-Factory/LLaMA-Factory/src/llmtuner/webui/runner.py\", line 245, in _preview\n",
      "    error = self._initialize(data, do_train, from_preview=True)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/Fine-Tuning-Mistral-7B-Using-Llama-Factory/LLaMA-Factory/src/llmtuner/webui/runner.py\", line 55, in _initialize\n",
      "    stage = TRAINING_STAGES[get(\"train.training_stage\")]\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/Fine-Tuning-Mistral-7B-Using-Llama-Factory/LLaMA-Factory/src/llmtuner/webui/runner.py\", line 52, in <lambda>\n",
      "    get = lambda name: data[self.manager.get_elem_by_name(name)]\n",
      "                       ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyError: dropdown\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/gradio/queueing.py\", line 407, in call_prediction\n",
      "    output = await route_utils.call_process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/gradio/route_utils.py\", line 226, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/gradio/blocks.py\", line 1550, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/gradio/blocks.py\", line 1199, in call_function\n",
      "    prediction = await utils.async_iteration(iterator)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/gradio/utils.py\", line 519, in async_iteration\n",
      "    return await iterator.__anext__()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/gradio/utils.py\", line 512, in __anext__\n",
      "    return await anyio.to_thread.run_sync(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 2144, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 851, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/gradio/utils.py\", line 495, in run_sync_iterator_async\n",
      "    return next(iterator)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/gradio/utils.py\", line 649, in gen_wrapper\n",
      "    yield from f(*args, **kwargs)\n",
      "  File \"/home/ubuntu/quamer/Fine-Tuning-Mistral-7B-Using-Llama-Factory/LLaMA-Factory/src/llmtuner/webui/runner.py\", line 276, in run_eval\n",
      "    yield from self._launch(data, do_train=False)\n",
      "  File \"/home/ubuntu/quamer/Fine-Tuning-Mistral-7B-Using-Llama-Factory/LLaMA-Factory/src/llmtuner/webui/runner.py\", line 254, in _launch\n",
      "    error = self._initialize(data, do_train, from_preview=False)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/Fine-Tuning-Mistral-7B-Using-Llama-Factory/LLaMA-Factory/src/llmtuner/webui/runner.py\", line 55, in _initialize\n",
      "    stage = TRAINING_STAGES[get(\"train.training_stage\")]\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/Fine-Tuning-Mistral-7B-Using-Llama-Factory/LLaMA-Factory/src/llmtuner/webui/runner.py\", line 52, in <lambda>\n",
      "    get = lambda name: data[self.manager.get_elem_by_name(name)]\n",
      "                       ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyError: dropdown\n",
      "[INFO|tokenization_utils_base.py:2046] 2024-03-13 11:20:12,552 >> loading file tokenizer.model from cache at /home/ubuntu/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/cf47bb3e18fe41a5351bc36eef76e9c900847c89/tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:2046] 2024-03-13 11:20:12,552 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2046] 2024-03-13 11:20:12,552 >> loading file special_tokens_map.json from cache at /home/ubuntu/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/cf47bb3e18fe41a5351bc36eef76e9c900847c89/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2046] 2024-03-13 11:20:12,552 >> loading file tokenizer_config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/cf47bb3e18fe41a5351bc36eef76e9c900847c89/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2046] 2024-03-13 11:20:12,552 >> loading file tokenizer.json from cache at /home/ubuntu/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/cf47bb3e18fe41a5351bc36eef76e9c900847c89/tokenizer.json\n",
      "[INFO|configuration_utils.py:728] 2024-03-13 11:20:12,607 >> loading configuration file config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/cf47bb3e18fe41a5351bc36eef76e9c900847c89/config.json\n",
      "[INFO|configuration_utils.py:791] 2024-03-13 11:20:12,607 >> Model config MistralConfig {\n",
      "  \"_name_or_path\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.38.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "03/13/2024 11:20:12 - INFO - llmtuner.model.patcher - Quantizing model to 4 bit.\n",
      "[INFO|modeling_utils.py:3257] 2024-03-13 11:20:12,615 >> loading weights file model.safetensors from cache at /home/ubuntu/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/cf47bb3e18fe41a5351bc36eef76e9c900847c89/model.safetensors.index.json\n",
      "[INFO|modeling_utils.py:1400] 2024-03-13 11:20:12,616 >> Instantiating MistralForCausalLM model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:845] 2024-03-13 11:20:12,617 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [00:02<00:00,  1.06it/s]\n",
      "[INFO|modeling_utils.py:3992] 2024-03-13 11:20:15,811 >> All model checkpoint weights were used when initializing MistralForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4000] 2024-03-13 11:20:15,811 >> All the weights of MistralForCausalLM were initialized from the model checkpoint at mistralai/Mistral-7B-Instruct-v0.2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MistralForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:800] 2024-03-13 11:20:15,833 >> loading configuration file generation_config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/cf47bb3e18fe41a5351bc36eef76e9c900847c89/generation_config.json\n",
      "[INFO|configuration_utils.py:845] 2024-03-13 11:20:15,834 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      "\n",
      "03/13/2024 11:20:16 - INFO - llmtuner.model.adapter - Fine-tuning method: LoRA\n",
      "03/13/2024 11:20:16 - INFO - llmtuner.model.adapter - Loaded adapter(s): saves/Custom/lora/train_2024-03-13-10-46-24\n",
      "03/13/2024 11:20:16 - INFO - llmtuner.model.loader - all params: 7245139968\n",
      "03/13/2024 11:20:16 - INFO - llmtuner.data.template - Add pad token: </s>\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 python src/train_web.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_VISIBLE_DEVICES=0 python src/train_bash.py \\\n",
    "    --stage sft \\\n",
    "    --do_train True \\\n",
    "    --model_name_or_path mistralai/Mistral-7B-Instruct-v0.2 \\\n",
    "    --finetuning_type lora \\\n",
    "    --quantization_bit 4 \\\n",
    "    --template mistral-instruction-v02 \\\n",
    "    --flash_attn True \\\n",
    "    --dataset_dir data \\\n",
    "    --dataset dockerNLCommands \\\n",
    "    --cutoff_len 512 \\\n",
    "    --learning_rate 0.0002 \\\n",
    "    --num_train_epochs 10.0 \\\n",
    "    --max_samples 10000 \\\n",
    "    --per_device_train_batch_size 64 \\\n",
    "    --gradient_accumulation_steps 8 \\\n",
    "    --lr_scheduler_type cosine \\\n",
    "    --max_grad_norm 0.3 \\\n",
    "    --logging_steps 5 \\\n",
    "    --save_steps 50 \\\n",
    "    --warmup_steps 0 \\\n",
    "    --optim adamw_torch \\\n",
    "    --output_dir saves/Custom/lora/train_2024-03-13-10-46-24 \\\n",
    "    --bf16 True \\\n",
    "    --lora_rank 8 \\\n",
    "    --lora_alpha 16 \\\n",
    "    --lora_dropout 0.1 \\\n",
    "    --lora_target q_proj, v_proj \\\n",
    "    --val_size 0.1 \\\n",
    "    --evaluation_strategy steps \\\n",
    "    --eval_steps 50 \\\n",
    "    --per_device_eval_batch_size 64 \\\n",
    "    --load_best_model_at_end True \\\n",
    "    --plot_loss True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we launch the web-version of llama-factory, we can access the gradio interface by visiting the URL shown in the terminal. We're provided with two types of URL, one local URL and another one is an external/public URL. We can use any of them to access the gradio interface. If you're using the google colab, you can only use the public URL, whereas if you're a Lambda Cloud system, you can use both the local and public URL.\n",
    "\n",
    "Let's now look at the llama-factory web interface.\n",
    "\n",
    "<img src=\"assets/gradio-interface.png\" alt=\"share=True\">\n",
    "\n",
    "Woah! That's a lot of options. We can see that we have a lot of options to choose from. We can select the model, the dataset, the hyperparameters, and the training options, etc. Everything is customizable. We can also see the training logs and the training progress in the web interface. This is a very powerful tool and can be used to train models on the cloud without any hassle and worry about the coding part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"assets/model config.png\" alt=\"share=True\">\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
