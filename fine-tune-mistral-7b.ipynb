{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clone Llama-Factory and Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/hiyouga/LLaMA-Factory.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/quamer/Fine-Tuning-Mistral-7B-Using-Llama-Factory/LLaMA-Factory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd LLaMA-Factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Llama-Factory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will run on localhost only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 python src/train_web.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to get an external URL, you'll need to set share=True in train_web.py like shown in the image below\n",
    "\n",
    "<img src=\"assets/train_web.png\" alt=\"share=True\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "dataset = {\n",
    "    'dockerNLCommands': {\n",
    "        \"hf_hub_url\": \"MattCoddity/dockerNLcommands\",\n",
    "        \"columns\": {\n",
    "            \"prompt\": \"instruction\",\n",
    "            \"query\": \"input\",\n",
    "            \"response\": \"output\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "template = {\n",
    "    _register_template(\n",
    "        name=\"mistral-instruction-v02\",\n",
    "        format_user=StringFormatter(slots=[\"[INST] {{content}} [/INST]\"]),\n",
    "        format_system=StringFormatter(slots=[{\"bos_token\"}, \"{{content}}\"]),\n",
    "        force_system=True,\n",
    ")\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://0.0.0.0:7860\n",
      "Running on public URL: https://f8916ac789600c46d7.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n",
      "03/13/2024 19:47:12 - INFO - llmtuner.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.bfloat16\n",
      "[INFO|tokenization_utils_base.py:2046] 2024-03-13 19:47:12,901 >> loading file tokenizer.model from cache at /home/ubuntu/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/cf47bb3e18fe41a5351bc36eef76e9c900847c89/tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:2046] 2024-03-13 19:47:12,901 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2046] 2024-03-13 19:47:12,901 >> loading file special_tokens_map.json from cache at /home/ubuntu/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/cf47bb3e18fe41a5351bc36eef76e9c900847c89/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2046] 2024-03-13 19:47:12,901 >> loading file tokenizer_config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/cf47bb3e18fe41a5351bc36eef76e9c900847c89/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2046] 2024-03-13 19:47:12,901 >> loading file tokenizer.json from cache at /home/ubuntu/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/cf47bb3e18fe41a5351bc36eef76e9c900847c89/tokenizer.json\n",
      "03/13/2024 19:47:12 - INFO - llmtuner.data.template - Add pad token: </s>\n",
      "03/13/2024 19:47:12 - INFO - llmtuner.data.loader - Loading dataset MattCoddity/dockerNLcommands...\n",
      "Running tokenizer on dataset: 100%|█| 2415/2415 [00:00<00:00, 4234.94 examples/s\n",
      "input_ids:\n",
      "[10649, 28747, 17824, 456, 12271, 297, 281, 14295, 3445, 13, 28777, 495, 528, 264, 1274, 302, 25399, 369, 506, 272, 500, 28726, 2794, 28718, 3469, 390, 652, 14014, 271, 28723, 13, 7226, 11143, 28747, 28705, 281, 14295, 12384, 1939, 4650, 464, 834, 374, 271, 28746, 437, 2794, 28718, 28742, 2]\n",
      "inputs:\n",
      " Human: translate this sentence in docker command\n",
      "Give me a list of containers that have the Ubuntu image as their ancestor.\n",
      "Assistant:  docker ps --filter 'ancestor=ubuntu'</s>\n",
      "label_ids:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 281, 14295, 12384, 1939, 4650, 464, 834, 374, 271, 28746, 437, 2794, 28718, 28742, 2]\n",
      "labels:\n",
      " docker ps --filter 'ancestor=ubuntu'</s>\n",
      "[INFO|configuration_utils.py:728] 2024-03-13 19:47:15,299 >> loading configuration file config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/cf47bb3e18fe41a5351bc36eef76e9c900847c89/config.json\n",
      "[INFO|configuration_utils.py:791] 2024-03-13 19:47:15,307 >> Model config MistralConfig {\n",
      "  \"_name_or_path\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.38.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3257] 2024-03-13 19:47:15,692 >> loading weights file model.safetensors from cache at /home/ubuntu/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/cf47bb3e18fe41a5351bc36eef76e9c900847c89/model.safetensors.index.json\n",
      "Downloading shards:   0%|                                 | 0/3 [00:00<?, ?it/s]\n",
      "model-00003-of-00003.safetensors:   0%|             | 0.00/4.54G [00:00<?, ?B/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:   1%|     | 31.5M/4.54G [00:00<00:15, 294MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:   2%|     | 73.4M/4.54G [00:00<00:13, 340MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:   3%|▏     | 115M/4.54G [00:00<00:12, 356MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:   3%|▏     | 157M/4.54G [00:00<00:12, 354MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:   4%|▎     | 199M/4.54G [00:00<00:12, 355MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:   5%|▎     | 241M/4.54G [00:00<00:11, 362MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:   6%|▎     | 283M/4.54G [00:00<00:11, 362MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:   7%|▍     | 325M/4.54G [00:00<00:11, 364MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:   8%|▍     | 367M/4.54G [00:01<00:11, 364MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:   9%|▌     | 409M/4.54G [00:01<00:11, 372MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  10%|▌     | 451M/4.54G [00:01<00:10, 375MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  11%|▋     | 493M/4.54G [00:01<00:10, 370MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  12%|▋     | 535M/4.54G [00:01<00:10, 370MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  13%|▊     | 577M/4.54G [00:01<00:10, 366MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  14%|▊     | 619M/4.54G [00:01<00:10, 367MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  15%|▊     | 661M/4.54G [00:01<00:10, 362MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  15%|▉     | 703M/4.54G [00:01<00:12, 317MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  16%|▉     | 744M/4.54G [00:02<00:12, 297MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  17%|█     | 776M/4.54G [00:02<00:13, 285MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  18%|█     | 807M/4.54G [00:02<00:13, 276MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  18%|█     | 839M/4.54G [00:02<00:13, 270MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  19%|█▏    | 870M/4.54G [00:02<00:13, 265MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  20%|█▏    | 902M/4.54G [00:02<00:13, 262MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  21%|█▏    | 933M/4.54G [00:02<00:13, 260MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  21%|█▎    | 965M/4.54G [00:03<00:13, 258MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  22%|█▎    | 996M/4.54G [00:03<00:13, 256MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  23%|█▏   | 1.03G/4.54G [00:03<00:13, 256MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  23%|█▏   | 1.06G/4.54G [00:03<00:13, 256MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  24%|█▏   | 1.09G/4.54G [00:03<00:13, 255MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  25%|█▏   | 1.12G/4.54G [00:03<00:13, 255MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  25%|█▎   | 1.15G/4.54G [00:03<00:13, 254MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  26%|█▎   | 1.18G/4.54G [00:03<00:13, 254MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  27%|█▎   | 1.22G/4.54G [00:04<00:13, 254MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  27%|█▎   | 1.25G/4.54G [00:04<00:12, 254MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  28%|█▍   | 1.28G/4.54G [00:04<00:12, 254MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  29%|█▍   | 1.31G/4.54G [00:04<00:12, 254MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  30%|█▍   | 1.34G/4.54G [00:04<00:12, 254MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  30%|█▌   | 1.37G/4.54G [00:04<00:12, 253MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  31%|█▌   | 1.41G/4.54G [00:04<00:12, 253MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  32%|█▌   | 1.44G/4.54G [00:04<00:12, 252MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  32%|█▌   | 1.47G/4.54G [00:05<00:12, 252MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  33%|█▋   | 1.50G/4.54G [00:05<00:12, 246MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  34%|█▋   | 1.53G/4.54G [00:05<00:12, 242MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  34%|█▋   | 1.56G/4.54G [00:05<00:12, 237MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  35%|█▊   | 1.59G/4.54G [00:05<00:11, 247MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  36%|█▊   | 1.63G/4.54G [00:05<00:12, 239MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  36%|█▊   | 1.66G/4.54G [00:05<00:11, 246MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  37%|█▊   | 1.69G/4.54G [00:05<00:11, 245MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  38%|█▉   | 1.72G/4.54G [00:06<00:11, 245MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  39%|█▉   | 1.75G/4.54G [00:06<00:11, 246MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  39%|█▉   | 1.78G/4.54G [00:06<00:11, 249MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  40%|█▉   | 1.81G/4.54G [00:06<00:11, 248MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  41%|██   | 1.85G/4.54G [00:06<00:10, 248MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  41%|██   | 1.88G/4.54G [00:06<00:11, 241MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  42%|██   | 1.91G/4.54G [00:06<00:11, 231MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  43%|██▏  | 1.94G/4.54G [00:06<00:10, 242MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  43%|██▏  | 1.97G/4.54G [00:07<00:10, 245MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  44%|██▏  | 2.00G/4.54G [00:07<00:10, 243MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  45%|██▏  | 2.03G/4.54G [00:07<00:10, 244MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  45%|██▎  | 2.07G/4.54G [00:07<00:10, 241MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  46%|██▎  | 2.10G/4.54G [00:07<00:10, 238MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  47%|██▎  | 2.13G/4.54G [00:07<00:09, 244MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  48%|██▍  | 2.16G/4.54G [00:07<00:09, 247MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  48%|██▍  | 2.19G/4.54G [00:07<00:09, 249MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  49%|██▍  | 2.22G/4.54G [00:08<00:09, 250MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  50%|██▍  | 2.25G/4.54G [00:08<00:09, 251MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  50%|██▌  | 2.29G/4.54G [00:08<00:08, 252MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  51%|██▌  | 2.32G/4.54G [00:08<00:08, 253MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  52%|██▌  | 2.35G/4.54G [00:08<00:08, 252MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  52%|██▌  | 2.38G/4.54G [00:08<00:08, 253MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  53%|██▋  | 2.41G/4.54G [00:08<00:08, 254MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  54%|██▋  | 2.44G/4.54G [00:08<00:08, 254MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  55%|██▋  | 2.47G/4.54G [00:09<00:08, 254MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  55%|██▊  | 2.51G/4.54G [00:09<00:08, 254MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  56%|██▊  | 2.54G/4.54G [00:09<00:07, 254MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  57%|██▊  | 2.57G/4.54G [00:09<00:07, 254MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  57%|██▊  | 2.60G/4.54G [00:09<00:07, 254MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  58%|██▉  | 2.63G/4.54G [00:09<00:07, 254MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  59%|██▉  | 2.66G/4.54G [00:09<00:07, 254MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  59%|██▉  | 2.69G/4.54G [00:09<00:07, 254MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  60%|███  | 2.73G/4.54G [00:10<00:10, 176MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  61%|███  | 2.77G/4.54G [00:10<00:08, 216MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  62%|███  | 2.81G/4.54G [00:10<00:06, 251MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  63%|███▏ | 2.85G/4.54G [00:10<00:05, 283MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  64%|███▏ | 2.89G/4.54G [00:10<00:05, 278MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  64%|███▏ | 2.93G/4.54G [00:10<00:05, 271MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  65%|███▎ | 2.96G/4.54G [00:11<00:05, 266MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  66%|███▎ | 2.99G/4.54G [00:11<00:05, 263MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  67%|███▎ | 3.02G/4.54G [00:11<00:05, 260MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  67%|███▎ | 3.05G/4.54G [00:11<00:05, 258MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  68%|███▍ | 3.08G/4.54G [00:11<00:05, 257MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  69%|███▍ | 3.11G/4.54G [00:11<00:05, 256MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  69%|███▍ | 3.15G/4.54G [00:11<00:05, 253MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  70%|███▍ | 3.18G/4.54G [00:11<00:05, 255MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  71%|███▌ | 3.21G/4.54G [00:12<00:05, 250MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  71%|███▌ | 3.24G/4.54G [00:12<00:05, 258MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  72%|███▌ | 3.27G/4.54G [00:12<00:05, 251MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  73%|███▋ | 3.30G/4.54G [00:12<00:04, 255MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  73%|███▋ | 3.33G/4.54G [00:12<00:04, 254MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  74%|███▋ | 3.37G/4.54G [00:12<00:04, 251MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  75%|███▋ | 3.40G/4.54G [00:12<00:04, 258MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  76%|███▊ | 3.43G/4.54G [00:12<00:04, 254MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  76%|███▊ | 3.46G/4.54G [00:13<00:04, 254MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  77%|███▊ | 3.49G/4.54G [00:13<00:04, 255MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  78%|███▉ | 3.52G/4.54G [00:13<00:03, 255MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  78%|███▉ | 3.55G/4.54G [00:13<00:03, 255MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  79%|███▉ | 3.59G/4.54G [00:13<00:03, 255MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  80%|███▉ | 3.62G/4.54G [00:13<00:03, 255MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  80%|████ | 3.65G/4.54G [00:13<00:03, 254MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  81%|████ | 3.68G/4.54G [00:13<00:03, 254MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  82%|████ | 3.71G/4.54G [00:13<00:03, 254MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  82%|████ | 3.74G/4.54G [00:14<00:03, 254MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  83%|████▏| 3.77G/4.54G [00:14<00:03, 254MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  84%|████▏| 3.81G/4.54G [00:14<00:02, 254MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  85%|████▏| 3.84G/4.54G [00:14<00:02, 254MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  85%|████▎| 3.87G/4.54G [00:14<00:02, 254MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  86%|████▎| 3.90G/4.54G [00:14<00:02, 254MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  87%|████▎| 3.93G/4.54G [00:14<00:02, 254MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  87%|████▎| 3.96G/4.54G [00:14<00:02, 254MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  88%|████▍| 4.00G/4.54G [00:15<00:02, 254MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  89%|████▍| 4.03G/4.54G [00:15<00:02, 254MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  89%|████▍| 4.06G/4.54G [00:15<00:01, 254MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  90%|████▌| 4.09G/4.54G [00:15<00:01, 254MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  91%|████▌| 4.12G/4.54G [00:15<00:01, 254MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  91%|████▌| 4.15G/4.54G [00:15<00:01, 254MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  92%|████▌| 4.18G/4.54G [00:15<00:01, 254MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  93%|████▋| 4.22G/4.54G [00:15<00:01, 254MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  94%|████▋| 4.25G/4.54G [00:16<00:01, 254MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  94%|████▋| 4.28G/4.54G [00:16<00:01, 254MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  95%|████▋| 4.31G/4.54G [00:16<00:00, 254MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  96%|████▊| 4.34G/4.54G [00:16<00:00, 254MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  96%|████▊| 4.37G/4.54G [00:16<00:00, 254MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  97%|████▊| 4.40G/4.54G [00:16<00:00, 254MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  98%|████▉| 4.44G/4.54G [00:16<00:00, 254MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  98%|████▉| 4.47G/4.54G [00:16<00:00, 254MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  99%|████▉| 4.50G/4.54G [00:17<00:00, 254MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors: 100%|█████| 4.54G/4.54G [00:17<00:00, 263MB/s]\u001b[A\n",
      "Downloading shards: 100%|█████████████████████████| 3/3 [00:17<00:00,  5.78s/it]\n",
      "[INFO|modeling_utils.py:1400] 2024-03-13 19:47:33,046 >> Instantiating MistralForCausalLM model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:845] 2024-03-13 19:47:33,047 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [00:02<00:00,  1.13it/s]\n",
      "[INFO|modeling_utils.py:3992] 2024-03-13 19:47:36,059 >> All model checkpoint weights were used when initializing MistralForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4000] 2024-03-13 19:47:36,059 >> All the weights of MistralForCausalLM were initialized from the model checkpoint at mistralai/Mistral-7B-Instruct-v0.2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MistralForCausalLM for predictions without further training.\n",
      "generation_config.json: 100%|██████████████████| 111/111 [00:00<00:00, 1.18MB/s]\n",
      "[INFO|configuration_utils.py:800] 2024-03-13 19:47:36,106 >> loading configuration file generation_config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/cf47bb3e18fe41a5351bc36eef76e9c900847c89/generation_config.json\n",
      "[INFO|configuration_utils.py:845] 2024-03-13 19:47:36,106 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      "\n",
      "03/13/2024 19:47:36 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.\n",
      "03/13/2024 19:47:36 - INFO - llmtuner.model.adapter - Fine-tuning method: LoRA\n",
      "03/13/2024 19:47:36 - INFO - llmtuner.model.loader - trainable params: 3407872 || all params: 7245139968 || trainable%: 0.0470\n",
      "[INFO|trainer.py:601] 2024-03-13 19:47:36,680 >> Using auto half precision backend\n",
      "[INFO|trainer.py:1812] 2024-03-13 19:47:36,826 >> ***** Running training *****\n",
      "[INFO|trainer.py:1813] 2024-03-13 19:47:36,826 >>   Num examples = 2,294\n",
      "[INFO|trainer.py:1814] 2024-03-13 19:47:36,826 >>   Num Epochs = 10\n",
      "[INFO|trainer.py:1815] 2024-03-13 19:47:36,826 >>   Instantaneous batch size per device = 16\n",
      "[INFO|trainer.py:1818] 2024-03-13 19:47:36,826 >>   Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "[INFO|trainer.py:1819] 2024-03-13 19:47:36,826 >>   Gradient Accumulation steps = 4\n",
      "[INFO|trainer.py:1820] 2024-03-13 19:47:36,826 >>   Total optimization steps = 360\n",
      "[INFO|trainer.py:1821] 2024-03-13 19:47:36,828 >>   Number of trainable parameters = 3,407,872\n",
      "03/13/2024 19:47:59 - INFO - llmtuner.extras.callbacks - {'loss': 1.7543, 'learning_rate': 4.9976e-05, 'epoch': 0.14}\n",
      "{'loss': 1.7543, 'grad_norm': 7.693007469177246, 'learning_rate': 4.997620553954645e-05, 'epoch': 0.14}\n",
      "03/13/2024 19:48:19 - INFO - llmtuner.extras.callbacks - {'loss': 1.0206, 'learning_rate': 4.9905e-05, 'epoch': 0.28}\n",
      "{'loss': 1.0206, 'grad_norm': 5.394199371337891, 'learning_rate': 4.990486745229364e-05, 'epoch': 0.28}\n",
      "03/13/2024 19:48:40 - INFO - llmtuner.extras.callbacks - {'loss': 0.5826, 'learning_rate': 4.9786e-05, 'epoch': 0.42}\n",
      "{'loss': 0.5826, 'grad_norm': 2.683964729309082, 'learning_rate': 4.9786121534345265e-05, 'epoch': 0.42}\n",
      "03/13/2024 19:49:00 - INFO - llmtuner.extras.callbacks - {'loss': 0.4099, 'learning_rate': 4.9620e-05, 'epoch': 0.56}\n",
      "{'loss': 0.4099, 'grad_norm': 1.8806061744689941, 'learning_rate': 4.962019382530521e-05, 'epoch': 0.56}\n",
      "03/13/2024 19:49:20 - INFO - llmtuner.extras.callbacks - {'loss': 0.3052, 'learning_rate': 4.9407e-05, 'epoch': 0.69}\n",
      "{'loss': 0.3052, 'grad_norm': 1.9219516515731812, 'learning_rate': 4.940740017799833e-05, 'epoch': 0.69}\n",
      "03/13/2024 19:49:42 - INFO - llmtuner.extras.callbacks - {'loss': 0.2440, 'learning_rate': 4.9148e-05, 'epoch': 0.83}\n",
      "{'loss': 0.244, 'grad_norm': 1.6164277791976929, 'learning_rate': 4.914814565722671e-05, 'epoch': 0.83}\n",
      "03/13/2024 19:50:03 - INFO - llmtuner.extras.callbacks - {'loss': 0.1924, 'learning_rate': 4.8843e-05, 'epoch': 0.97}\n",
      "{'loss': 0.1924, 'grad_norm': 1.8378784656524658, 'learning_rate': 4.884292376870567e-05, 'epoch': 0.97}\n",
      "03/13/2024 19:50:24 - INFO - llmtuner.extras.callbacks - {'loss': 0.1655, 'learning_rate': 4.8492e-05, 'epoch': 1.11}\n",
      "{'loss': 0.1655, 'grad_norm': 2.151557445526123, 'learning_rate': 4.849231551964771e-05, 'epoch': 1.11}\n",
      "03/13/2024 19:50:45 - INFO - llmtuner.extras.callbacks - {'loss': 0.1203, 'learning_rate': 4.8097e-05, 'epoch': 1.25}\n",
      "{'loss': 0.1203, 'grad_norm': 1.2347593307495117, 'learning_rate': 4.8096988312782174e-05, 'epoch': 1.25}\n",
      "03/13/2024 19:51:05 - INFO - llmtuner.extras.callbacks - {'loss': 0.1188, 'learning_rate': 4.7658e-05, 'epoch': 1.39}\n",
      "{'loss': 0.1188, 'grad_norm': 2.1008129119873047, 'learning_rate': 4.765769467591625e-05, 'epoch': 1.39}\n",
      "03/13/2024 19:51:26 - INFO - llmtuner.extras.callbacks - {'loss': 0.0852, 'learning_rate': 4.7175e-05, 'epoch': 1.53}\n",
      "{'loss': 0.0852, 'grad_norm': 1.351984977722168, 'learning_rate': 4.717527082945554e-05, 'epoch': 1.53}\n",
      "03/13/2024 19:51:48 - INFO - llmtuner.extras.callbacks - {'loss': 0.0854, 'learning_rate': 4.6651e-05, 'epoch': 1.67}\n",
      "{'loss': 0.0854, 'grad_norm': 1.8272274732589722, 'learning_rate': 4.665063509461097e-05, 'epoch': 1.67}\n",
      "03/13/2024 19:52:10 - INFO - llmtuner.extras.callbacks - {'loss': 0.0672, 'learning_rate': 4.6085e-05, 'epoch': 1.81}\n",
      "{'loss': 0.0672, 'grad_norm': 1.3354312181472778, 'learning_rate': 4.608478614532215e-05, 'epoch': 1.81}\n",
      "03/13/2024 19:52:31 - INFO - llmtuner.extras.callbacks - {'loss': 0.0455, 'learning_rate': 4.5479e-05, 'epoch': 1.94}\n",
      "{'loss': 0.0455, 'grad_norm': 1.7812442779541016, 'learning_rate': 4.54788011072248e-05, 'epoch': 1.94}\n",
      "03/13/2024 19:52:51 - INFO - llmtuner.extras.callbacks - {'loss': 0.0366, 'learning_rate': 4.4834e-05, 'epoch': 2.08}\n",
      "{'loss': 0.0366, 'grad_norm': 1.6480517387390137, 'learning_rate': 4.4833833507280884e-05, 'epoch': 2.08}\n",
      "03/13/2024 19:53:13 - INFO - llmtuner.extras.callbacks - {'loss': 0.0503, 'learning_rate': 4.4151e-05, 'epoch': 2.22}\n",
      "{'loss': 0.0503, 'grad_norm': 1.5116844177246094, 'learning_rate': 4.415111107797445e-05, 'epoch': 2.22}\n",
      "03/13/2024 19:53:34 - INFO - llmtuner.extras.callbacks - {'loss': 0.0429, 'learning_rate': 4.3432e-05, 'epoch': 2.36}\n",
      "{'loss': 0.0429, 'grad_norm': 1.792582631111145, 'learning_rate': 4.34319334202531e-05, 'epoch': 2.36}\n",
      "03/13/2024 19:53:56 - INFO - llmtuner.extras.callbacks - {'loss': 0.0482, 'learning_rate': 4.2678e-05, 'epoch': 2.50}\n",
      "{'loss': 0.0482, 'grad_norm': 2.4267895221710205, 'learning_rate': 4.267766952966369e-05, 'epoch': 2.5}\n",
      "03/13/2024 19:54:18 - INFO - llmtuner.extras.callbacks - {'loss': 0.0317, 'learning_rate': 4.1890e-05, 'epoch': 2.64}\n",
      "{'loss': 0.0317, 'grad_norm': 0.7215550541877747, 'learning_rate': 4.188975519039151e-05, 'epoch': 2.64}\n",
      "03/13/2024 19:54:39 - INFO - llmtuner.extras.callbacks - {'loss': 0.0372, 'learning_rate': 4.1070e-05, 'epoch': 2.78}\n",
      "{'loss': 0.0372, 'grad_norm': 1.289726972579956, 'learning_rate': 4.1069690242163484e-05, 'epoch': 2.78}\n",
      "[INFO|trainer.py:3376] 2024-03-13 19:54:39,716 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3378] 2024-03-13 19:54:39,716 >>   Num examples = 121\n",
      "[INFO|trainer.py:3381] 2024-03-13 19:54:39,716 >>   Batch size = 16\n",
      "03/13/2024 19:54:42 - INFO - llmtuner.extras.callbacks - {'loss': 0.0000, 'learning_rate': 0.0000e+00, 'epoch': 2.78}\n",
      "{'eval_loss': 0.03319624438881874, 'eval_runtime': 2.4078, 'eval_samples_per_second': 50.252, 'eval_steps_per_second': 3.322, 'epoch': 2.78}\n",
      "[INFO|trainer.py:3067] 2024-03-13 19:54:42,147 >> Saving model checkpoint to saves/Custom/lora/train_2024-03-13-19-46-19/tmp-checkpoint-100\n",
      "[INFO|configuration_utils.py:728] 2024-03-13 19:54:42,269 >> loading configuration file config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/cf47bb3e18fe41a5351bc36eef76e9c900847c89/config.json\n",
      "[INFO|configuration_utils.py:791] 2024-03-13 19:54:42,270 >> Model config MistralConfig {\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.38.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2459] 2024-03-13 19:54:42,419 >> tokenizer config file saved in saves/Custom/lora/train_2024-03-13-19-46-19/tmp-checkpoint-100/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2468] 2024-03-13 19:54:42,429 >> Special tokens file saved in saves/Custom/lora/train_2024-03-13-19-46-19/tmp-checkpoint-100/special_tokens_map.json\n",
      "03/13/2024 19:55:03 - INFO - llmtuner.extras.callbacks - {'loss': 0.0245, 'learning_rate': 4.0219e-05, 'epoch': 2.92}\n",
      "{'loss': 0.0245, 'grad_norm': 1.3112704753875732, 'learning_rate': 4.021903572521802e-05, 'epoch': 2.92}\n",
      "03/13/2024 19:55:24 - INFO - llmtuner.extras.callbacks - {'loss': 0.0333, 'learning_rate': 3.9339e-05, 'epoch': 3.06}\n",
      "{'loss': 0.0333, 'grad_norm': 1.9846090078353882, 'learning_rate': 3.933941090877615e-05, 'epoch': 3.06}\n",
      "03/13/2024 19:55:44 - INFO - llmtuner.extras.callbacks - {'loss': 0.0351, 'learning_rate': 3.8432e-05, 'epoch': 3.19}\n",
      "{'loss': 0.0351, 'grad_norm': 2.320976734161377, 'learning_rate': 3.84324902086706e-05, 'epoch': 3.19}\n",
      "03/13/2024 19:56:05 - INFO - llmtuner.extras.callbacks - {'loss': 0.0261, 'learning_rate': 3.7500e-05, 'epoch': 3.33}\n",
      "{'loss': 0.0261, 'grad_norm': 2.042772054672241, 'learning_rate': 3.7500000000000003e-05, 'epoch': 3.33}\n",
      "03/13/2024 19:56:26 - INFO - llmtuner.extras.callbacks - {'loss': 0.0204, 'learning_rate': 3.6544e-05, 'epoch': 3.47}\n",
      "{'loss': 0.0204, 'grad_norm': 2.495635747909546, 'learning_rate': 3.654371533087586e-05, 'epoch': 3.47}\n",
      "03/13/2024 19:56:48 - INFO - llmtuner.extras.callbacks - {'loss': 0.0122, 'learning_rate': 3.5565e-05, 'epoch': 3.61}\n",
      "{'loss': 0.0122, 'grad_norm': 0.8619056344032288, 'learning_rate': 3.556545654351749e-05, 'epoch': 3.61}\n",
      "03/13/2024 19:57:08 - INFO - llmtuner.extras.callbacks - {'loss': 0.0288, 'learning_rate': 3.4567e-05, 'epoch': 3.75}\n",
      "{'loss': 0.0288, 'grad_norm': 1.8066715002059937, 'learning_rate': 3.456708580912725e-05, 'epoch': 3.75}\n",
      "03/13/2024 19:57:30 - INFO - llmtuner.extras.callbacks - {'loss': 0.0222, 'learning_rate': 3.3551e-05, 'epoch': 3.89}\n",
      "{'loss': 0.0222, 'grad_norm': 3.5700998306274414, 'learning_rate': 3.355050358314172e-05, 'epoch': 3.89}\n",
      "03/13/2024 19:57:51 - INFO - llmtuner.extras.callbacks - {'loss': 0.0249, 'learning_rate': 3.2518e-05, 'epoch': 4.03}\n",
      "{'loss': 0.0249, 'grad_norm': 1.5312789678573608, 'learning_rate': 3.251764498760683e-05, 'epoch': 4.03}\n",
      "03/13/2024 19:58:12 - INFO - llmtuner.extras.callbacks - {'loss': 0.0227, 'learning_rate': 3.1470e-05, 'epoch': 4.17}\n",
      "{'loss': 0.0227, 'grad_norm': 0.45907583832740784, 'learning_rate': 3.147047612756302e-05, 'epoch': 4.17}\n",
      "03/13/2024 19:58:33 - INFO - llmtuner.extras.callbacks - {'loss': 0.0171, 'learning_rate': 3.0411e-05, 'epoch': 4.31}\n",
      "{'loss': 0.0171, 'grad_norm': 1.6312272548675537, 'learning_rate': 3.0410990348452573e-05, 'epoch': 4.31}\n",
      "03/13/2024 19:58:55 - INFO - llmtuner.extras.callbacks - {'loss': 0.0169, 'learning_rate': 2.9341e-05, 'epoch': 4.44}\n",
      "{'loss': 0.0169, 'grad_norm': 2.005723476409912, 'learning_rate': 2.9341204441673266e-05, 'epoch': 4.44}\n",
      "03/13/2024 19:59:16 - INFO - llmtuner.extras.callbacks - {'loss': 0.0192, 'learning_rate': 2.8263e-05, 'epoch': 4.58}\n",
      "{'loss': 0.0192, 'grad_norm': 1.4385231733322144, 'learning_rate': 2.8263154805501297e-05, 'epoch': 4.58}\n",
      "03/13/2024 19:59:37 - INFO - llmtuner.extras.callbacks - {'loss': 0.0136, 'learning_rate': 2.7179e-05, 'epoch': 4.72}\n",
      "{'loss': 0.0136, 'grad_norm': 1.0173711776733398, 'learning_rate': 2.717889356869146e-05, 'epoch': 4.72}\n",
      "03/13/2024 19:59:59 - INFO - llmtuner.extras.callbacks - {'loss': 0.0122, 'learning_rate': 2.6090e-05, 'epoch': 4.86}\n",
      "{'loss': 0.0122, 'grad_norm': 1.2668229341506958, 'learning_rate': 2.6090484684133404e-05, 'epoch': 4.86}\n",
      "03/13/2024 20:00:20 - INFO - llmtuner.extras.callbacks - {'loss': 0.0120, 'learning_rate': 2.5000e-05, 'epoch': 5.00}\n",
      "{'loss': 0.012, 'grad_norm': 1.913548469543457, 'learning_rate': 2.5e-05, 'epoch': 5.0}\n",
      "03/13/2024 20:00:42 - INFO - llmtuner.extras.callbacks - {'loss': 0.0128, 'learning_rate': 2.3910e-05, 'epoch': 5.14}\n",
      "{'loss': 0.0128, 'grad_norm': 2.155971050262451, 'learning_rate': 2.3909515315866605e-05, 'epoch': 5.14}\n",
      "03/13/2024 20:01:04 - INFO - llmtuner.extras.callbacks - {'loss': 0.0127, 'learning_rate': 2.2821e-05, 'epoch': 5.28}\n",
      "{'loss': 0.0127, 'grad_norm': 1.0692299604415894, 'learning_rate': 2.2821106431308544e-05, 'epoch': 5.28}\n",
      "03/13/2024 20:01:25 - INFO - llmtuner.extras.callbacks - {'loss': 0.0158, 'learning_rate': 2.1737e-05, 'epoch': 5.42}\n",
      "{'loss': 0.0158, 'grad_norm': 0.9680432081222534, 'learning_rate': 2.173684519449872e-05, 'epoch': 5.42}\n",
      "03/13/2024 20:01:47 - INFO - llmtuner.extras.callbacks - {'loss': 0.0125, 'learning_rate': 2.0659e-05, 'epoch': 5.56}\n",
      "{'loss': 0.0125, 'grad_norm': 1.0661903619766235, 'learning_rate': 2.0658795558326743e-05, 'epoch': 5.56}\n",
      "[INFO|trainer.py:3376] 2024-03-13 20:01:47,253 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3378] 2024-03-13 20:01:47,253 >>   Num examples = 121\n",
      "[INFO|trainer.py:3381] 2024-03-13 20:01:47,253 >>   Batch size = 16\n",
      "03/13/2024 20:01:49 - INFO - llmtuner.extras.callbacks - {'loss': 0.0000, 'learning_rate': 0.0000e+00, 'epoch': 5.56}\n",
      "{'eval_loss': 0.01622922532260418, 'eval_runtime': 2.4123, 'eval_samples_per_second': 50.161, 'eval_steps_per_second': 3.316, 'epoch': 5.56}\n",
      "[INFO|trainer.py:3067] 2024-03-13 20:01:49,675 >> Saving model checkpoint to saves/Custom/lora/train_2024-03-13-19-46-19/tmp-checkpoint-200\n",
      "[INFO|configuration_utils.py:728] 2024-03-13 20:01:49,781 >> loading configuration file config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/cf47bb3e18fe41a5351bc36eef76e9c900847c89/config.json\n",
      "[INFO|configuration_utils.py:791] 2024-03-13 20:01:49,782 >> Model config MistralConfig {\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.38.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2459] 2024-03-13 20:01:49,914 >> tokenizer config file saved in saves/Custom/lora/train_2024-03-13-19-46-19/tmp-checkpoint-200/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2468] 2024-03-13 20:01:49,922 >> Special tokens file saved in saves/Custom/lora/train_2024-03-13-19-46-19/tmp-checkpoint-200/special_tokens_map.json\n",
      "03/13/2024 20:02:11 - INFO - llmtuner.extras.callbacks - {'loss': 0.0081, 'learning_rate': 1.9589e-05, 'epoch': 5.69}\n",
      "{'loss': 0.0081, 'grad_norm': 0.8271896243095398, 'learning_rate': 1.958900965154743e-05, 'epoch': 5.69}\n",
      "03/13/2024 20:02:31 - INFO - llmtuner.extras.callbacks - {'loss': 0.0125, 'learning_rate': 1.8530e-05, 'epoch': 5.83}\n",
      "{'loss': 0.0125, 'grad_norm': 1.7405743598937988, 'learning_rate': 1.852952387243698e-05, 'epoch': 5.83}\n",
      "03/13/2024 20:02:53 - INFO - llmtuner.extras.callbacks - {'loss': 0.0104, 'learning_rate': 1.7482e-05, 'epoch': 5.97}\n",
      "{'loss': 0.0104, 'grad_norm': 1.2488248348236084, 'learning_rate': 1.7482355012393177e-05, 'epoch': 5.97}\n",
      "03/13/2024 20:03:14 - INFO - llmtuner.extras.callbacks - {'loss': 0.0070, 'learning_rate': 1.6449e-05, 'epoch': 6.11}\n",
      "{'loss': 0.007, 'grad_norm': 0.5560044050216675, 'learning_rate': 1.6449496416858284e-05, 'epoch': 6.11}\n",
      "03/13/2024 20:03:35 - INFO - llmtuner.extras.callbacks - {'loss': 0.0120, 'learning_rate': 1.5433e-05, 'epoch': 6.25}\n",
      "{'loss': 0.012, 'grad_norm': 1.4551178216934204, 'learning_rate': 1.5432914190872757e-05, 'epoch': 6.25}\n",
      "03/13/2024 20:03:55 - INFO - llmtuner.extras.callbacks - {'loss': 0.0132, 'learning_rate': 1.4435e-05, 'epoch': 6.39}\n",
      "{'loss': 0.0132, 'grad_norm': 1.6263498067855835, 'learning_rate': 1.443454345648252e-05, 'epoch': 6.39}\n",
      "03/13/2024 20:04:17 - INFO - llmtuner.extras.callbacks - {'loss': 0.0042, 'learning_rate': 1.3456e-05, 'epoch': 6.53}\n",
      "{'loss': 0.0042, 'grad_norm': 0.22708508372306824, 'learning_rate': 1.3456284669124158e-05, 'epoch': 6.53}\n",
      "03/13/2024 20:04:38 - INFO - llmtuner.extras.callbacks - {'loss': 0.0088, 'learning_rate': 1.2500e-05, 'epoch': 6.67}\n",
      "{'loss': 0.0088, 'grad_norm': 0.6515786051750183, 'learning_rate': 1.2500000000000006e-05, 'epoch': 6.67}\n",
      "03/13/2024 20:05:00 - INFO - llmtuner.extras.callbacks - {'loss': 0.0041, 'learning_rate': 1.1568e-05, 'epoch': 6.81}\n",
      "{'loss': 0.0041, 'grad_norm': 0.4397377669811249, 'learning_rate': 1.1567509791329401e-05, 'epoch': 6.81}\n",
      "03/13/2024 20:05:22 - INFO - llmtuner.extras.callbacks - {'loss': 0.0102, 'learning_rate': 1.0661e-05, 'epoch': 6.94}\n",
      "{'loss': 0.0102, 'grad_norm': 1.5320991277694702, 'learning_rate': 1.0660589091223855e-05, 'epoch': 6.94}\n",
      "03/13/2024 20:05:42 - INFO - llmtuner.extras.callbacks - {'loss': 0.0067, 'learning_rate': 9.7810e-06, 'epoch': 7.08}\n",
      "{'loss': 0.0067, 'grad_norm': 1.1604887247085571, 'learning_rate': 9.780964274781984e-06, 'epoch': 7.08}\n",
      "03/13/2024 20:06:03 - INFO - llmtuner.extras.callbacks - {'loss': 0.0037, 'learning_rate': 8.9303e-06, 'epoch': 7.22}\n",
      "{'loss': 0.0037, 'grad_norm': 1.5971548557281494, 'learning_rate': 8.930309757836517e-06, 'epoch': 7.22}\n",
      "03/13/2024 20:06:25 - INFO - llmtuner.extras.callbacks - {'loss': 0.0067, 'learning_rate': 8.1102e-06, 'epoch': 7.36}\n",
      "{'loss': 0.0067, 'grad_norm': 0.9654971361160278, 'learning_rate': 8.110244809608495e-06, 'epoch': 7.36}\n",
      "03/13/2024 20:06:47 - INFO - llmtuner.extras.callbacks - {'loss': 0.0058, 'learning_rate': 7.3223e-06, 'epoch': 7.50}\n",
      "{'loss': 0.0058, 'grad_norm': 2.1204416751861572, 'learning_rate': 7.3223304703363135e-06, 'epoch': 7.5}\n",
      "03/13/2024 20:07:08 - INFO - llmtuner.extras.callbacks - {'loss': 0.0059, 'learning_rate': 6.5681e-06, 'epoch': 7.64}\n",
      "{'loss': 0.0059, 'grad_norm': 0.866258442401886, 'learning_rate': 6.568066579746901e-06, 'epoch': 7.64}\n",
      "03/13/2024 20:07:29 - INFO - llmtuner.extras.callbacks - {'loss': 0.0094, 'learning_rate': 5.8489e-06, 'epoch': 7.78}\n",
      "{'loss': 0.0094, 'grad_norm': 0.6335222125053406, 'learning_rate': 5.848888922025553e-06, 'epoch': 7.78}\n",
      "03/13/2024 20:07:50 - INFO - llmtuner.extras.callbacks - {'loss': 0.0090, 'learning_rate': 5.1662e-06, 'epoch': 7.92}\n",
      "{'loss': 0.009, 'grad_norm': 0.981534481048584, 'learning_rate': 5.166166492719124e-06, 'epoch': 7.92}\n",
      "03/13/2024 20:08:11 - INFO - llmtuner.extras.callbacks - {'loss': 0.0063, 'learning_rate': 4.5212e-06, 'epoch': 8.06}\n",
      "{'loss': 0.0063, 'grad_norm': 0.859342098236084, 'learning_rate': 4.521198892775203e-06, 'epoch': 8.06}\n",
      "03/13/2024 20:08:33 - INFO - llmtuner.extras.callbacks - {'loss': 0.0034, 'learning_rate': 3.9152e-06, 'epoch': 8.19}\n",
      "{'loss': 0.0034, 'grad_norm': 0.47514280676841736, 'learning_rate': 3.9152138546778625e-06, 'epoch': 8.19}\n",
      "03/13/2024 20:08:54 - INFO - llmtuner.extras.callbacks - {'loss': 0.0050, 'learning_rate': 3.3494e-06, 'epoch': 8.33}\n",
      "{'loss': 0.005, 'grad_norm': 1.3065800666809082, 'learning_rate': 3.3493649053890326e-06, 'epoch': 8.33}\n",
      "[INFO|trainer.py:3376] 2024-03-13 20:08:54,388 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3378] 2024-03-13 20:08:54,388 >>   Num examples = 121\n",
      "[INFO|trainer.py:3381] 2024-03-13 20:08:54,388 >>   Batch size = 16\n",
      "03/13/2024 20:08:56 - INFO - llmtuner.extras.callbacks - {'loss': 0.0000, 'learning_rate': 0.0000e+00, 'epoch': 8.33}\n",
      "{'eval_loss': 0.02027682214975357, 'eval_runtime': 2.411, 'eval_samples_per_second': 50.188, 'eval_steps_per_second': 3.318, 'epoch': 8.33}\n",
      "[INFO|trainer.py:3067] 2024-03-13 20:08:56,814 >> Saving model checkpoint to saves/Custom/lora/train_2024-03-13-19-46-19/tmp-checkpoint-300\n",
      "[INFO|configuration_utils.py:728] 2024-03-13 20:08:56,929 >> loading configuration file config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/cf47bb3e18fe41a5351bc36eef76e9c900847c89/config.json\n",
      "[INFO|configuration_utils.py:791] 2024-03-13 20:08:56,930 >> Model config MistralConfig {\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.38.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2459] 2024-03-13 20:08:57,073 >> tokenizer config file saved in saves/Custom/lora/train_2024-03-13-19-46-19/tmp-checkpoint-300/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2468] 2024-03-13 20:08:57,081 >> Special tokens file saved in saves/Custom/lora/train_2024-03-13-19-46-19/tmp-checkpoint-300/special_tokens_map.json\n",
      "03/13/2024 20:09:18 - INFO - llmtuner.extras.callbacks - {'loss': 0.0031, 'learning_rate': 2.8247e-06, 'epoch': 8.47}\n",
      "{'loss': 0.0031, 'grad_norm': 0.5928324460983276, 'learning_rate': 2.8247291705444575e-06, 'epoch': 8.47}\n",
      "03/13/2024 20:09:40 - INFO - llmtuner.extras.callbacks - {'loss': 0.0042, 'learning_rate': 2.3423e-06, 'epoch': 8.61}\n",
      "{'loss': 0.0042, 'grad_norm': 0.5617555975914001, 'learning_rate': 2.3423053240837515e-06, 'epoch': 8.61}\n",
      "03/13/2024 20:10:01 - INFO - llmtuner.extras.callbacks - {'loss': 0.0031, 'learning_rate': 1.9030e-06, 'epoch': 8.75}\n",
      "{'loss': 0.0031, 'grad_norm': 0.668617844581604, 'learning_rate': 1.9030116872178316e-06, 'epoch': 8.75}\n",
      "03/13/2024 20:10:21 - INFO - llmtuner.extras.callbacks - {'loss': 0.0071, 'learning_rate': 1.5077e-06, 'epoch': 8.89}\n",
      "{'loss': 0.0071, 'grad_norm': 0.8139355182647705, 'learning_rate': 1.5076844803522922e-06, 'epoch': 8.89}\n",
      "03/13/2024 20:10:42 - INFO - llmtuner.extras.callbacks - {'loss': 0.0078, 'learning_rate': 1.1571e-06, 'epoch': 9.03}\n",
      "{'loss': 0.0078, 'grad_norm': 1.4223390817642212, 'learning_rate': 1.1570762312943295e-06, 'epoch': 9.03}\n",
      "03/13/2024 20:11:03 - INFO - llmtuner.extras.callbacks - {'loss': 0.0014, 'learning_rate': 8.5185e-07, 'epoch': 9.17}\n",
      "{'loss': 0.0014, 'grad_norm': 0.5418530702590942, 'learning_rate': 8.51854342773295e-07, 'epoch': 9.17}\n",
      "03/13/2024 20:11:26 - INFO - llmtuner.extras.callbacks - {'loss': 0.0025, 'learning_rate': 5.9260e-07, 'epoch': 9.31}\n",
      "{'loss': 0.0025, 'grad_norm': 0.22671547532081604, 'learning_rate': 5.925998220016659e-07, 'epoch': 9.31}\n",
      "03/13/2024 20:11:47 - INFO - llmtuner.extras.callbacks - {'loss': 0.0058, 'learning_rate': 3.7981e-07, 'epoch': 9.44}\n",
      "{'loss': 0.0058, 'grad_norm': 0.6007376909255981, 'learning_rate': 3.7980617469479953e-07, 'epoch': 9.44}\n",
      "03/13/2024 20:12:08 - INFO - llmtuner.extras.callbacks - {'loss': 0.0063, 'learning_rate': 2.1388e-07, 'epoch': 9.58}\n",
      "{'loss': 0.0063, 'grad_norm': 0.395778089761734, 'learning_rate': 2.1387846565474045e-07, 'epoch': 9.58}\n",
      "03/13/2024 20:12:29 - INFO - llmtuner.extras.callbacks - {'loss': 0.0036, 'learning_rate': 9.5133e-08, 'epoch': 9.72}\n",
      "{'loss': 0.0036, 'grad_norm': 0.9250916242599487, 'learning_rate': 9.513254770636137e-08, 'epoch': 9.72}\n",
      "03/13/2024 20:12:50 - INFO - llmtuner.extras.callbacks - {'loss': 0.0038, 'learning_rate': 2.3794e-08, 'epoch': 9.86}\n",
      "{'loss': 0.0038, 'grad_norm': 0.47003576159477234, 'learning_rate': 2.3794460453555047e-08, 'epoch': 9.86}\n",
      "03/13/2024 20:13:10 - INFO - llmtuner.extras.callbacks - {'loss': 0.0084, 'learning_rate': 0.0000e+00, 'epoch': 10.00}\n",
      "{'loss': 0.0084, 'grad_norm': 0.8105049133300781, 'learning_rate': 0.0, 'epoch': 10.0}\n",
      "[INFO|trainer.py:2067] 2024-03-13 20:13:10,447 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "[INFO|trainer.py:2271] 2024-03-13 20:13:10,448 >> Loading best model from saves/Custom/lora/train_2024-03-13-19-46-19/checkpoint-200 (score: 0.01622922532260418).\n",
      "03/13/2024 20:13:10 - INFO - llmtuner.extras.callbacks - {'loss': 0.0000, 'learning_rate': 0.0000e+00, 'epoch': 10.00}\n",
      "{'train_runtime': 1533.7769, 'train_samples_per_second': 14.957, 'train_steps_per_second': 0.235, 'train_loss': 0.0840026685127264, 'epoch': 10.0}\n",
      "[INFO|trainer.py:3067] 2024-03-13 20:13:10,611 >> Saving model checkpoint to saves/Custom/lora/train_2024-03-13-19-46-19\n",
      "[INFO|configuration_utils.py:728] 2024-03-13 20:13:10,879 >> loading configuration file config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/cf47bb3e18fe41a5351bc36eef76e9c900847c89/config.json\n",
      "[INFO|configuration_utils.py:791] 2024-03-13 20:13:10,880 >> Model config MistralConfig {\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.38.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2459] 2024-03-13 20:13:11,025 >> tokenizer config file saved in saves/Custom/lora/train_2024-03-13-19-46-19/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2468] 2024-03-13 20:13:11,033 >> Special tokens file saved in saves/Custom/lora/train_2024-03-13-19-46-19/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =       10.0\n",
      "  train_loss               =      0.084\n",
      "  train_runtime            = 0:25:33.77\n",
      "  train_samples_per_second =     14.957\n",
      "  train_steps_per_second   =      0.235\n",
      "[INFO|trainer.py:3376] 2024-03-13 20:13:11,102 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3378] 2024-03-13 20:13:11,102 >>   Num examples = 121\n",
      "[INFO|trainer.py:3381] 2024-03-13 20:13:11,102 >>   Batch size = 16\n",
      "03/13/2024 20:13:13 - INFO - llmtuner.extras.callbacks - {'loss': 0.0000, 'learning_rate': 0.0000e+00, 'epoch': 10.00}\n",
      "{'eval_loss': 0.01622922532260418, 'eval_runtime': 2.41, 'eval_samples_per_second': 50.208, 'eval_steps_per_second': 3.32, 'epoch': 10.0}\n",
      "***** eval metrics *****\n",
      "  epoch                   =       10.0\n",
      "  eval_loss               =     0.0162\n",
      "  eval_runtime            = 0:00:02.41\n",
      "  eval_samples_per_second =     50.208\n",
      "  eval_steps_per_second   =       3.32\n",
      "[INFO|modelcard.py:450] 2024-03-13 20:13:13,541 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n",
      "[INFO|tokenization_utils_base.py:2046] 2024-03-13 20:13:44,441 >> loading file tokenizer.model from cache at /home/ubuntu/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/cf47bb3e18fe41a5351bc36eef76e9c900847c89/tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:2046] 2024-03-13 20:13:44,441 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2046] 2024-03-13 20:13:44,441 >> loading file special_tokens_map.json from cache at /home/ubuntu/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/cf47bb3e18fe41a5351bc36eef76e9c900847c89/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2046] 2024-03-13 20:13:44,441 >> loading file tokenizer_config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/cf47bb3e18fe41a5351bc36eef76e9c900847c89/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2046] 2024-03-13 20:13:44,441 >> loading file tokenizer.json from cache at /home/ubuntu/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/cf47bb3e18fe41a5351bc36eef76e9c900847c89/tokenizer.json\n",
      "[INFO|configuration_utils.py:728] 2024-03-13 20:13:44,546 >> loading configuration file config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/cf47bb3e18fe41a5351bc36eef76e9c900847c89/config.json\n",
      "[INFO|configuration_utils.py:791] 2024-03-13 20:13:44,547 >> Model config MistralConfig {\n",
      "  \"_name_or_path\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.38.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "03/13/2024 20:13:44 - INFO - llmtuner.model.patcher - Quantizing model to 4 bit.\n",
      "[INFO|modeling_utils.py:3257] 2024-03-13 20:13:44,554 >> loading weights file model.safetensors from cache at /home/ubuntu/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/cf47bb3e18fe41a5351bc36eef76e9c900847c89/model.safetensors.index.json\n",
      "[INFO|modeling_utils.py:1400] 2024-03-13 20:13:44,555 >> Instantiating MistralForCausalLM model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:845] 2024-03-13 20:13:44,556 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [00:03<00:00,  1.11s/it]\n",
      "[INFO|modeling_utils.py:3992] 2024-03-13 20:13:48,169 >> All model checkpoint weights were used when initializing MistralForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4000] 2024-03-13 20:13:48,169 >> All the weights of MistralForCausalLM were initialized from the model checkpoint at mistralai/Mistral-7B-Instruct-v0.2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MistralForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:800] 2024-03-13 20:13:48,240 >> loading configuration file generation_config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/cf47bb3e18fe41a5351bc36eef76e9c900847c89/generation_config.json\n",
      "[INFO|configuration_utils.py:845] 2024-03-13 20:13:48,241 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      "\n",
      "03/13/2024 20:13:48 - INFO - llmtuner.model.adapter - Fine-tuning method: LoRA\n",
      "03/13/2024 20:13:48 - INFO - llmtuner.model.adapter - Loaded adapter(s): saves/Custom/lora/train_2024-03-13-19-46-19\n",
      "03/13/2024 20:13:48 - INFO - llmtuner.model.loader - all params: 7245139968\n",
      "03/13/2024 20:13:48 - INFO - llmtuner.data.template - Add pad token: </s>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/gradio/queueing.py\", line 407, in call_prediction\n",
      "    output = await route_utils.call_process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/gradio/route_utils.py\", line 226, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/gradio/blocks.py\", line 1550, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/gradio/blocks.py\", line 1199, in call_function\n",
      "    prediction = await utils.async_iteration(iterator)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/gradio/utils.py\", line 519, in async_iteration\n",
      "    return await iterator.__anext__()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/gradio/utils.py\", line 512, in __anext__\n",
      "    return await anyio.to_thread.run_sync(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 2144, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 851, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/gradio/utils.py\", line 495, in run_sync_iterator_async\n",
      "    return next(iterator)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/gradio/utils.py\", line 649, in gen_wrapper\n",
      "    yield from f(*args, **kwargs)\n",
      "  File \"/home/ubuntu/quamer/Fine-Tuning-Mistral-7B-Using-Llama-Factory/LLaMA-Factory/src/llmtuner/webui/runner.py\", line 270, in preview_eval\n",
      "    yield from self._preview(data, do_train=False)\n",
      "  File \"/home/ubuntu/quamer/Fine-Tuning-Mistral-7B-Using-Llama-Factory/LLaMA-Factory/src/llmtuner/webui/runner.py\", line 245, in _preview\n",
      "    error = self._initialize(data, do_train, from_preview=True)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/Fine-Tuning-Mistral-7B-Using-Llama-Factory/LLaMA-Factory/src/llmtuner/webui/runner.py\", line 55, in _initialize\n",
      "    stage = TRAINING_STAGES[get(\"train.training_stage\")]\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/Fine-Tuning-Mistral-7B-Using-Llama-Factory/LLaMA-Factory/src/llmtuner/webui/runner.py\", line 52, in <lambda>\n",
      "    get = lambda name: data[self.manager.get_elem_by_name(name)]\n",
      "                       ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyError: dropdown\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/gradio/queueing.py\", line 407, in call_prediction\n",
      "    output = await route_utils.call_process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/gradio/route_utils.py\", line 226, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/gradio/blocks.py\", line 1550, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/gradio/blocks.py\", line 1199, in call_function\n",
      "    prediction = await utils.async_iteration(iterator)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/gradio/utils.py\", line 519, in async_iteration\n",
      "    return await iterator.__anext__()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/gradio/utils.py\", line 512, in __anext__\n",
      "    return await anyio.to_thread.run_sync(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 2144, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 851, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/gradio/utils.py\", line 495, in run_sync_iterator_async\n",
      "    return next(iterator)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/gradio/utils.py\", line 649, in gen_wrapper\n",
      "    yield from f(*args, **kwargs)\n",
      "  File \"/home/ubuntu/quamer/Fine-Tuning-Mistral-7B-Using-Llama-Factory/LLaMA-Factory/src/llmtuner/webui/runner.py\", line 276, in run_eval\n",
      "    yield from self._launch(data, do_train=False)\n",
      "  File \"/home/ubuntu/quamer/Fine-Tuning-Mistral-7B-Using-Llama-Factory/LLaMA-Factory/src/llmtuner/webui/runner.py\", line 254, in _launch\n",
      "    error = self._initialize(data, do_train, from_preview=False)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/Fine-Tuning-Mistral-7B-Using-Llama-Factory/LLaMA-Factory/src/llmtuner/webui/runner.py\", line 55, in _initialize\n",
      "    stage = TRAINING_STAGES[get(\"train.training_stage\")]\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/Fine-Tuning-Mistral-7B-Using-Llama-Factory/LLaMA-Factory/src/llmtuner/webui/runner.py\", line 52, in <lambda>\n",
      "    get = lambda name: data[self.manager.get_elem_by_name(name)]\n",
      "                       ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyError: dropdown\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/gradio/queueing.py\", line 407, in call_prediction\n",
      "    output = await route_utils.call_process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/gradio/route_utils.py\", line 226, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/gradio/blocks.py\", line 1550, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/gradio/blocks.py\", line 1199, in call_function\n",
      "    prediction = await utils.async_iteration(iterator)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/gradio/utils.py\", line 519, in async_iteration\n",
      "    return await iterator.__anext__()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/gradio/utils.py\", line 512, in __anext__\n",
      "    return await anyio.to_thread.run_sync(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 2144, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 851, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/gradio/utils.py\", line 495, in run_sync_iterator_async\n",
      "    return next(iterator)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/gradio/utils.py\", line 649, in gen_wrapper\n",
      "    yield from f(*args, **kwargs)\n",
      "  File \"/home/ubuntu/quamer/Fine-Tuning-Mistral-7B-Using-Llama-Factory/LLaMA-Factory/src/llmtuner/webui/components/export.py\", line 71, in save_model\n",
      "    export_model(args)\n",
      "  File \"/home/ubuntu/quamer/Fine-Tuning-Mistral-7B-Using-Llama-Factory/LLaMA-Factory/src/llmtuner/train/tuner.py\", line 50, in export_model\n",
      "    raise ValueError(\"Please merge adapters before quantizing the model.\")\n",
      "ValueError: Please merge adapters before quantizing the model.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/gradio/queueing.py\", line 407, in call_prediction\n",
      "    output = await route_utils.call_process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/gradio/route_utils.py\", line 226, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/gradio/blocks.py\", line 1550, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/gradio/blocks.py\", line 1199, in call_function\n",
      "    prediction = await utils.async_iteration(iterator)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/gradio/utils.py\", line 519, in async_iteration\n",
      "    return await iterator.__anext__()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/gradio/utils.py\", line 512, in __anext__\n",
      "    return await anyio.to_thread.run_sync(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 2144, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 851, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/gradio/utils.py\", line 495, in run_sync_iterator_async\n",
      "    return next(iterator)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/gradio/utils.py\", line 649, in gen_wrapper\n",
      "    yield from f(*args, **kwargs)\n",
      "  File \"/home/ubuntu/quamer/Fine-Tuning-Mistral-7B-Using-Llama-Factory/LLaMA-Factory/src/llmtuner/webui/components/export.py\", line 71, in save_model\n",
      "    export_model(args)\n",
      "  File \"/home/ubuntu/quamer/Fine-Tuning-Mistral-7B-Using-Llama-Factory/LLaMA-Factory/src/llmtuner/train/tuner.py\", line 50, in export_model\n",
      "    raise ValueError(\"Please merge adapters before quantizing the model.\")\n",
      "ValueError: Please merge adapters before quantizing the model.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/gradio/queueing.py\", line 407, in call_prediction\n",
      "    output = await route_utils.call_process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/gradio/route_utils.py\", line 226, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/gradio/blocks.py\", line 1550, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/gradio/blocks.py\", line 1199, in call_function\n",
      "    prediction = await utils.async_iteration(iterator)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/gradio/utils.py\", line 519, in async_iteration\n",
      "    return await iterator.__anext__()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/gradio/utils.py\", line 512, in __anext__\n",
      "    return await anyio.to_thread.run_sync(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 2144, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 851, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/gradio/utils.py\", line 495, in run_sync_iterator_async\n",
      "    return next(iterator)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/gradio/utils.py\", line 649, in gen_wrapper\n",
      "    yield from f(*args, **kwargs)\n",
      "  File \"/home/ubuntu/quamer/Fine-Tuning-Mistral-7B-Using-Llama-Factory/LLaMA-Factory/src/llmtuner/webui/components/export.py\", line 71, in save_model\n",
      "    export_model(args)\n",
      "  File \"/home/ubuntu/quamer/Fine-Tuning-Mistral-7B-Using-Llama-Factory/LLaMA-Factory/src/llmtuner/train/tuner.py\", line 50, in export_model\n",
      "    raise ValueError(\"Please merge adapters before quantizing the model.\")\n",
      "ValueError: Please merge adapters before quantizing the model.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/gradio/queueing.py\", line 407, in call_prediction\n",
      "    output = await route_utils.call_process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/gradio/route_utils.py\", line 226, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/gradio/blocks.py\", line 1550, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/gradio/blocks.py\", line 1199, in call_function\n",
      "    prediction = await utils.async_iteration(iterator)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/gradio/utils.py\", line 519, in async_iteration\n",
      "    return await iterator.__anext__()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/gradio/utils.py\", line 512, in __anext__\n",
      "    return await anyio.to_thread.run_sync(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 2144, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 851, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/gradio/utils.py\", line 495, in run_sync_iterator_async\n",
      "    return next(iterator)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/gradio/utils.py\", line 649, in gen_wrapper\n",
      "    yield from f(*args, **kwargs)\n",
      "  File \"/home/ubuntu/quamer/Fine-Tuning-Mistral-7B-Using-Llama-Factory/LLaMA-Factory/src/llmtuner/webui/runner.py\", line 270, in preview_eval\n",
      "    yield from self._preview(data, do_train=False)\n",
      "  File \"/home/ubuntu/quamer/Fine-Tuning-Mistral-7B-Using-Llama-Factory/LLaMA-Factory/src/llmtuner/webui/runner.py\", line 245, in _preview\n",
      "    error = self._initialize(data, do_train, from_preview=True)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/Fine-Tuning-Mistral-7B-Using-Llama-Factory/LLaMA-Factory/src/llmtuner/webui/runner.py\", line 55, in _initialize\n",
      "    stage = TRAINING_STAGES[get(\"train.training_stage\")]\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/Fine-Tuning-Mistral-7B-Using-Llama-Factory/LLaMA-Factory/src/llmtuner/webui/runner.py\", line 52, in <lambda>\n",
      "    get = lambda name: data[self.manager.get_elem_by_name(name)]\n",
      "                       ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyError: dropdown\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/gradio/queueing.py\", line 407, in call_prediction\n",
      "    output = await route_utils.call_process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/gradio/route_utils.py\", line 226, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/gradio/blocks.py\", line 1550, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/gradio/blocks.py\", line 1199, in call_function\n",
      "    prediction = await utils.async_iteration(iterator)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/gradio/utils.py\", line 519, in async_iteration\n",
      "    return await iterator.__anext__()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/gradio/utils.py\", line 512, in __anext__\n",
      "    return await anyio.to_thread.run_sync(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 2144, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 851, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/gradio/utils.py\", line 495, in run_sync_iterator_async\n",
      "    return next(iterator)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/gradio/utils.py\", line 649, in gen_wrapper\n",
      "    yield from f(*args, **kwargs)\n",
      "  File \"/home/ubuntu/quamer/Fine-Tuning-Mistral-7B-Using-Llama-Factory/LLaMA-Factory/src/llmtuner/webui/runner.py\", line 270, in preview_eval\n",
      "    yield from self._preview(data, do_train=False)\n",
      "  File \"/home/ubuntu/quamer/Fine-Tuning-Mistral-7B-Using-Llama-Factory/LLaMA-Factory/src/llmtuner/webui/runner.py\", line 245, in _preview\n",
      "    error = self._initialize(data, do_train, from_preview=True)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/Fine-Tuning-Mistral-7B-Using-Llama-Factory/LLaMA-Factory/src/llmtuner/webui/runner.py\", line 55, in _initialize\n",
      "    stage = TRAINING_STAGES[get(\"train.training_stage\")]\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/Fine-Tuning-Mistral-7B-Using-Llama-Factory/LLaMA-Factory/src/llmtuner/webui/runner.py\", line 52, in <lambda>\n",
      "    get = lambda name: data[self.manager.get_elem_by_name(name)]\n",
      "                       ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyError: dropdown\n",
      "[INFO|training_args.py:1902] 2024-03-13 20:37:41,870 >> PyTorch: setting up devices\n",
      "[INFO|training_args.py:1611] 2024-03-13 20:37:41,871 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "03/13/2024 20:37:41 - WARNING - llmtuner.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.\n",
      "03/13/2024 20:37:41 - INFO - llmtuner.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.bfloat16\n",
      "tokenizer_config.json: 100%|███████████████| 1.47k/1.47k [00:00<00:00, 15.7MB/s]\n",
      "tokenizer.model: 100%|████████████████████████| 493k/493k [00:00<00:00, 131MB/s]\n",
      "special_tokens_map.json: 100%|████████████████| 72.0/72.0 [00:00<00:00, 926kB/s]\n",
      "tokenizer.json: 100%|██████████████████████| 1.80M/1.80M [00:00<00:00, 28.9MB/s]\n",
      "[INFO|tokenization_utils_base.py:2046] 2024-03-13 20:37:42,638 >> loading file tokenizer.model from cache at /home/ubuntu/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.1/snapshots/73068f3702d050a2fd5aa2ca1e612e5036429398/tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:2046] 2024-03-13 20:37:42,639 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2046] 2024-03-13 20:37:42,639 >> loading file special_tokens_map.json from cache at /home/ubuntu/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.1/snapshots/73068f3702d050a2fd5aa2ca1e612e5036429398/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2046] 2024-03-13 20:37:42,639 >> loading file tokenizer_config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.1/snapshots/73068f3702d050a2fd5aa2ca1e612e5036429398/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2046] 2024-03-13 20:37:42,639 >> loading file tokenizer.json from cache at /home/ubuntu/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.1/snapshots/73068f3702d050a2fd5aa2ca1e612e5036429398/tokenizer.json\n",
      "03/13/2024 20:37:42 - INFO - llmtuner.data.template - Add pad token: </s>\n",
      "03/13/2024 20:37:42 - INFO - llmtuner.data.loader - Loading dataset MattCoddity/dockerNLcommands...\n",
      "Running tokenizer on dataset: 100%|█| 2415/2415 [00:00<00:00, 4547.31 examples/s\n",
      "input_ids:\n",
      "[1, 733, 16289, 28793, 17824, 456, 12271, 297, 281, 14295, 3445, 13, 28777, 495, 528, 264, 1274, 302, 25399, 369, 506, 272, 500, 28726, 2794, 28718, 3469, 390, 652, 14014, 271, 28723, 733, 28748, 16289, 28793, 281, 14295, 12384, 1939, 4650, 464, 834, 374, 271, 28746, 437, 2794, 28718, 28742, 2]\n",
      "inputs:\n",
      "<s> [INST] translate this sentence in docker command\n",
      "Give me a list of containers that have the Ubuntu image as their ancestor. [/INST] docker ps --filter 'ancestor=ubuntu'</s>\n",
      "label_ids:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 281, 14295, 12384, 1939, 4650, 464, 834, 374, 271, 28746, 437, 2794, 28718, 28742, 2]\n",
      "labels:\n",
      " docker ps --filter 'ancestor=ubuntu'</s>\n",
      "config.json: 100%|█████████████████████████████| 571/571 [00:00<00:00, 6.09MB/s]\n",
      "[INFO|configuration_utils.py:728] 2024-03-13 20:37:44,167 >> loading configuration file config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.1/snapshots/73068f3702d050a2fd5aa2ca1e612e5036429398/config.json\n",
      "[INFO|configuration_utils.py:791] 2024-03-13 20:37:44,168 >> Model config MistralConfig {\n",
      "  \"_name_or_path\": \"mistralai/Mistral-7B-Instruct-v0.1\",\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 4096,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.38.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "03/13/2024 20:37:44 - INFO - llmtuner.model.patcher - Quantizing model to 4 bit.\n",
      "model.safetensors.index.json: 100%|█████████| 25.1k/25.1k [00:00<00:00, 138MB/s]\n",
      "[INFO|modeling_utils.py:3257] 2024-03-13 20:37:44,432 >> loading weights file model.safetensors from cache at /home/ubuntu/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.1/snapshots/73068f3702d050a2fd5aa2ca1e612e5036429398/model.safetensors.index.json\n",
      "Downloading shards:   0%|                                 | 0/2 [00:00<?, ?it/s]\n",
      "model-00001-of-00002.safetensors:   0%|             | 0.00/9.94G [00:00<?, ?B/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   0%|     | 31.5M/9.94G [00:00<00:35, 280MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   1%|     | 62.9M/9.94G [00:00<00:34, 289MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   1%|      | 105M/9.94G [00:00<00:29, 328MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   1%|      | 147M/9.94G [00:00<00:29, 329MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   2%|      | 189M/9.94G [00:00<00:27, 350MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   2%|▏     | 231M/9.94G [00:00<00:27, 356MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   3%|▏     | 273M/9.94G [00:00<00:26, 361MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   3%|▏     | 315M/9.94G [00:00<00:25, 376MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   4%|▏     | 357M/9.94G [00:01<00:25, 372MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   4%|▏     | 398M/9.94G [00:01<00:25, 373MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   4%|▎     | 440M/9.94G [00:01<00:25, 370MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   5%|▎     | 482M/9.94G [00:01<00:25, 366MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   5%|▎     | 524M/9.94G [00:01<00:25, 369MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   6%|▎     | 566M/9.94G [00:01<00:25, 373MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   6%|▎     | 608M/9.94G [00:01<00:24, 376MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   7%|▍     | 650M/9.94G [00:01<00:28, 326MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   7%|▍     | 692M/9.94G [00:02<00:30, 302MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   7%|▍     | 724M/9.94G [00:02<00:31, 288MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   8%|▍     | 755M/9.94G [00:02<00:32, 279MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   8%|▍     | 786M/9.94G [00:02<00:33, 271MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   8%|▍     | 818M/9.94G [00:02<00:34, 267MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   9%|▌     | 849M/9.94G [00:02<00:34, 263MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   9%|▌     | 881M/9.94G [00:02<00:34, 260MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   9%|▌     | 912M/9.94G [00:02<00:34, 258MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   9%|▌     | 944M/9.94G [00:03<00:35, 256MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  10%|▌     | 975M/9.94G [00:03<00:35, 256MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  10%|▌    | 1.01G/9.94G [00:03<00:34, 255MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  10%|▌    | 1.04G/9.94G [00:03<00:34, 255MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  11%|▌    | 1.07G/9.94G [00:03<00:34, 255MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  11%|▌    | 1.10G/9.94G [00:03<00:34, 255MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  11%|▌    | 1.13G/9.94G [00:03<00:34, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  12%|▌    | 1.16G/9.94G [00:03<00:34, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  12%|▌    | 1.20G/9.94G [00:03<00:34, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  12%|▌    | 1.23G/9.94G [00:04<00:34, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  13%|▋    | 1.26G/9.94G [00:04<00:34, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  13%|▋    | 1.29G/9.94G [00:04<00:34, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  13%|▋    | 1.32G/9.94G [00:04<00:33, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  14%|▋    | 1.35G/9.94G [00:04<00:33, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  14%|▋    | 1.38G/9.94G [00:04<00:33, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  14%|▋    | 1.42G/9.94G [00:04<00:33, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  15%|▋    | 1.45G/9.94G [00:04<00:33, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  15%|▋    | 1.48G/9.94G [00:05<00:33, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  15%|▊    | 1.51G/9.94G [00:05<00:33, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  16%|▊    | 1.54G/9.94G [00:05<00:33, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  16%|▊    | 1.57G/9.94G [00:05<00:32, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  16%|▊    | 1.60G/9.94G [00:05<00:32, 253MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  16%|▊    | 1.64G/9.94G [00:05<00:32, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  17%|▊    | 1.67G/9.94G [00:05<00:32, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  17%|▊    | 1.70G/9.94G [00:05<00:32, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  17%|▊    | 1.73G/9.94G [00:06<00:32, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  18%|▉    | 1.76G/9.94G [00:06<00:32, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  18%|▉    | 1.79G/9.94G [00:06<00:32, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  18%|▉    | 1.82G/9.94G [00:06<00:32, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  19%|▉    | 1.86G/9.94G [00:06<00:31, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  19%|▉    | 1.89G/9.94G [00:06<00:31, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  19%|▉    | 1.92G/9.94G [00:06<00:31, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  20%|▉    | 1.95G/9.94G [00:06<00:31, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  20%|▉    | 1.98G/9.94G [00:07<00:31, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  20%|█    | 2.01G/9.94G [00:07<00:31, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  21%|█    | 2.04G/9.94G [00:07<00:31, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  21%|█    | 2.08G/9.94G [00:07<00:30, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  21%|█    | 2.11G/9.94G [00:07<00:30, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  22%|█    | 2.14G/9.94G [00:07<00:30, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  22%|█    | 2.17G/9.94G [00:07<00:30, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  22%|█    | 2.20G/9.94G [00:07<00:30, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  22%|█    | 2.23G/9.94G [00:08<00:30, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  23%|█▏   | 2.26G/9.94G [00:08<00:30, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  23%|█▏   | 2.30G/9.94G [00:08<00:30, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  23%|█▏   | 2.33G/9.94G [00:08<00:29, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  24%|█▏   | 2.36G/9.94G [00:08<00:29, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  24%|█▏   | 2.39G/9.94G [00:08<00:29, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  24%|█▏   | 2.42G/9.94G [00:08<00:29, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  25%|█▏   | 2.45G/9.94G [00:08<00:29, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  25%|█▏   | 2.49G/9.94G [00:09<00:29, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  25%|█▎   | 2.52G/9.94G [00:09<00:29, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  26%|█▎   | 2.55G/9.94G [00:09<00:29, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  26%|█▎   | 2.58G/9.94G [00:09<00:28, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  26%|█▎   | 2.61G/9.94G [00:09<00:28, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  27%|█▎   | 2.64G/9.94G [00:09<00:28, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  27%|█▎   | 2.67G/9.94G [00:09<00:28, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  27%|█▎   | 2.71G/9.94G [00:09<00:28, 253MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  28%|█▍   | 2.74G/9.94G [00:10<00:28, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  28%|█▍   | 2.77G/9.94G [00:10<00:30, 238MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  28%|█▍   | 2.80G/9.94G [00:10<00:27, 256MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  28%|█▍   | 2.83G/9.94G [00:10<00:27, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  29%|█▍   | 2.86G/9.94G [00:10<00:27, 255MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  29%|█▍   | 2.89G/9.94G [00:10<00:27, 255MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  29%|█▍   | 2.93G/9.94G [00:10<00:27, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  30%|█▍   | 2.96G/9.94G [00:10<00:27, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  30%|█▌   | 2.99G/9.94G [00:11<00:27, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  30%|█▌   | 3.02G/9.94G [00:11<00:27, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  31%|█▌   | 3.05G/9.94G [00:11<00:27, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  31%|█▌   | 3.08G/9.94G [00:11<00:27, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  31%|█▌   | 3.11G/9.94G [00:11<00:26, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  32%|█▌   | 3.15G/9.94G [00:11<00:26, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  32%|█▌   | 3.18G/9.94G [00:11<00:26, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  32%|█▌   | 3.21G/9.94G [00:11<00:26, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  33%|█▋   | 3.24G/9.94G [00:12<00:26, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  33%|█▋   | 3.27G/9.94G [00:12<00:26, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  33%|█▋   | 3.30G/9.94G [00:12<00:26, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  34%|█▋   | 3.33G/9.94G [00:12<00:26, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  34%|█▋   | 3.37G/9.94G [00:12<00:25, 253MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  34%|█▋   | 3.40G/9.94G [00:12<00:25, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  34%|█▋   | 3.43G/9.94G [00:12<00:25, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  35%|█▋   | 3.46G/9.94G [00:12<00:25, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  35%|█▊   | 3.49G/9.94G [00:13<00:25, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  35%|█▊   | 3.52G/9.94G [00:13<00:25, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  36%|█▊   | 3.55G/9.94G [00:13<00:25, 253MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  36%|█▊   | 3.59G/9.94G [00:13<00:25, 253MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  36%|█▊   | 3.62G/9.94G [00:13<00:25, 253MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  37%|█▊   | 3.65G/9.94G [00:13<00:24, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  37%|█▊   | 3.68G/9.94G [00:13<00:24, 253MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  37%|█▊   | 3.71G/9.94G [00:13<00:24, 253MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  38%|█▉   | 3.74G/9.94G [00:14<00:24, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  38%|█▉   | 3.77G/9.94G [00:14<00:24, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  38%|█▉   | 3.81G/9.94G [00:14<00:24, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  39%|█▉   | 3.84G/9.94G [00:14<00:24, 253MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  39%|█▉   | 3.87G/9.94G [00:14<00:23, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  39%|█▉   | 3.90G/9.94G [00:14<00:23, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  40%|█▉   | 3.93G/9.94G [00:14<00:23, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  40%|█▉   | 3.96G/9.94G [00:14<00:23, 253MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  40%|██   | 4.00G/9.94G [00:15<00:23, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  40%|██   | 4.03G/9.94G [00:15<00:23, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  41%|██   | 4.06G/9.94G [00:15<00:23, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  41%|██   | 4.09G/9.94G [00:15<00:23, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  41%|██   | 4.12G/9.94G [00:15<00:22, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  42%|██   | 4.15G/9.94G [00:15<00:22, 253MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  42%|██   | 4.18G/9.94G [00:15<00:22, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  42%|██   | 4.22G/9.94G [00:15<00:22, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  43%|██▏  | 4.25G/9.94G [00:16<00:22, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  43%|██▏  | 4.28G/9.94G [00:16<00:22, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  43%|██▏  | 4.31G/9.94G [00:16<00:22, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  44%|██▏  | 4.34G/9.94G [00:16<00:22, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  44%|██▏  | 4.37G/9.94G [00:16<00:21, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  44%|██▏  | 4.40G/9.94G [00:16<00:21, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  45%|██▏  | 4.44G/9.94G [00:16<00:21, 253MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  45%|██▏  | 4.47G/9.94G [00:16<00:21, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  45%|██▎  | 4.50G/9.94G [00:17<00:21, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  46%|██▎  | 4.53G/9.94G [00:17<00:21, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  46%|██▎  | 4.56G/9.94G [00:17<00:21, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  46%|██▎  | 4.59G/9.94G [00:17<00:21, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  47%|██▎  | 4.62G/9.94G [00:17<00:20, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  47%|██▎  | 4.66G/9.94G [00:17<00:20, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  47%|██▎  | 4.69G/9.94G [00:17<00:20, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  47%|██▎  | 4.72G/9.94G [00:17<00:20, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  48%|██▍  | 4.75G/9.94G [00:18<00:20, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  48%|██▍  | 4.78G/9.94G [00:18<00:20, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  48%|██▍  | 4.81G/9.94G [00:18<00:20, 255MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  49%|██▍  | 4.84G/9.94G [00:18<00:20, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  49%|██▍  | 4.88G/9.94G [00:18<00:19, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  49%|██▍  | 4.91G/9.94G [00:18<00:19, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  50%|██▍  | 4.94G/9.94G [00:18<00:19, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  50%|██▍  | 4.97G/9.94G [00:18<00:19, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  50%|██▌  | 5.00G/9.94G [00:18<00:19, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  51%|██▌  | 5.03G/9.94G [00:19<00:19, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  51%|██▌  | 5.06G/9.94G [00:19<00:19, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  51%|██▌  | 5.10G/9.94G [00:19<00:19, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  52%|██▌  | 5.13G/9.94G [00:19<00:18, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  52%|██▌  | 5.16G/9.94G [00:19<00:18, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  52%|██▌  | 5.19G/9.94G [00:19<00:18, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  53%|██▋  | 5.22G/9.94G [00:19<00:18, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  53%|██▋  | 5.25G/9.94G [00:19<00:18, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  53%|██▋  | 5.28G/9.94G [00:20<00:18, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  53%|██▋  | 5.32G/9.94G [00:20<00:18, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  54%|██▋  | 5.35G/9.94G [00:20<00:18, 242MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  54%|██▋  | 5.38G/9.94G [00:20<00:18, 252MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  54%|██▋  | 5.41G/9.94G [00:20<00:18, 249MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  55%|██▋  | 5.44G/9.94G [00:20<00:17, 253MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  55%|██▊  | 5.47G/9.94G [00:20<00:17, 256MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  55%|██▊  | 5.51G/9.94G [00:20<00:17, 255MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  56%|██▊  | 5.54G/9.94G [00:21<00:17, 255MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  56%|██▊  | 5.57G/9.94G [00:21<00:17, 255MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  56%|██▊  | 5.60G/9.94G [00:21<00:17, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  57%|██▊  | 5.63G/9.94G [00:21<00:16, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  57%|██▊  | 5.66G/9.94G [00:21<00:16, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  57%|██▊  | 5.69G/9.94G [00:21<00:16, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  58%|██▉  | 5.73G/9.94G [00:21<00:16, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  58%|██▉  | 5.76G/9.94G [00:21<00:16, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  58%|██▉  | 5.79G/9.94G [00:22<00:16, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  59%|██▉  | 5.82G/9.94G [00:22<00:16, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  59%|██▉  | 5.85G/9.94G [00:22<00:16, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  59%|██▉  | 5.88G/9.94G [00:22<00:15, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  59%|██▉  | 5.91G/9.94G [00:22<00:15, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  60%|██▉  | 5.95G/9.94G [00:22<00:15, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  60%|███  | 5.98G/9.94G [00:22<00:15, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  60%|███  | 6.01G/9.94G [00:22<00:15, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  61%|███  | 6.04G/9.94G [00:23<00:15, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  61%|███  | 6.07G/9.94G [00:23<00:29, 132MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  61%|███  | 6.11G/9.94G [00:23<00:22, 170MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  62%|███  | 6.16G/9.94G [00:23<00:18, 208MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  62%|███  | 6.20G/9.94G [00:23<00:15, 241MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  63%|███▏ | 6.24G/9.94G [00:24<00:13, 272MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  63%|███▏ | 6.28G/9.94G [00:24<00:12, 296MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  64%|███▏ | 6.32G/9.94G [00:24<00:11, 308MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  64%|███▏ | 6.36G/9.94G [00:24<00:12, 289MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  64%|███▏ | 6.40G/9.94G [00:24<00:12, 280MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  65%|███▏ | 6.43G/9.94G [00:24<00:12, 273MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  65%|███▏ | 6.46G/9.94G [00:24<00:13, 267MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  65%|███▎ | 6.49G/9.94G [00:24<00:13, 263MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  66%|███▎ | 6.52G/9.94G [00:25<00:13, 261MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  66%|███▎ | 6.55G/9.94G [00:25<00:13, 258MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  66%|███▎ | 6.59G/9.94G [00:25<00:13, 257MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  67%|███▎ | 6.62G/9.94G [00:25<00:12, 256MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  67%|███▎ | 6.65G/9.94G [00:25<00:12, 256MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  67%|███▎ | 6.68G/9.94G [00:25<00:12, 255MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  67%|███▎ | 6.71G/9.94G [00:25<00:12, 255MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  68%|███▍ | 6.74G/9.94G [00:25<00:12, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  68%|███▍ | 6.77G/9.94G [00:26<00:12, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  68%|███▍ | 6.81G/9.94G [00:26<00:12, 255MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  69%|███▍ | 6.84G/9.94G [00:26<00:12, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  69%|███▍ | 6.87G/9.94G [00:26<00:12, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  69%|███▍ | 6.90G/9.94G [00:26<00:11, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  70%|███▍ | 6.93G/9.94G [00:26<00:11, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  70%|███▌ | 6.96G/9.94G [00:26<00:11, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  70%|███▌ | 6.99G/9.94G [00:26<00:11, 253MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  71%|███▌ | 7.03G/9.94G [00:27<00:11, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  71%|███▌ | 7.06G/9.94G [00:27<00:11, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  71%|███▌ | 7.09G/9.94G [00:27<00:11, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  72%|███▌ | 7.12G/9.94G [00:27<00:11, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  72%|███▌ | 7.15G/9.94G [00:27<00:10, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  72%|███▌ | 7.18G/9.94G [00:27<00:10, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  73%|███▋ | 7.21G/9.94G [00:27<00:10, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  73%|███▋ | 7.25G/9.94G [00:27<00:10, 253MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  73%|███▋ | 7.28G/9.94G [00:28<00:10, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  74%|███▋ | 7.31G/9.94G [00:28<00:10, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  74%|███▋ | 7.34G/9.94G [00:28<00:10, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  74%|███▋ | 7.37G/9.94G [00:28<00:10, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  74%|███▋ | 7.40G/9.94G [00:28<00:10, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  75%|███▋ | 7.43G/9.94G [00:28<00:09, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  75%|███▊ | 7.47G/9.94G [00:28<00:09, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  75%|███▊ | 7.50G/9.94G [00:28<00:09, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  76%|███▊ | 7.53G/9.94G [00:29<00:09, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  76%|███▊ | 7.56G/9.94G [00:29<00:09, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  76%|███▊ | 7.59G/9.94G [00:29<00:09, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  77%|███▊ | 7.62G/9.94G [00:29<00:09, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  77%|███▊ | 7.65G/9.94G [00:29<00:09, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  77%|███▊ | 7.69G/9.94G [00:29<00:08, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  78%|███▉ | 7.72G/9.94G [00:29<00:08, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  78%|███▉ | 7.75G/9.94G [00:29<00:08, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  78%|███▉ | 7.78G/9.94G [00:30<00:08, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  79%|███▉ | 7.81G/9.94G [00:30<00:08, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  79%|███▉ | 7.84G/9.94G [00:30<00:08, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  79%|███▉ | 7.87G/9.94G [00:30<00:08, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  80%|███▉ | 7.91G/9.94G [00:30<00:08, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  80%|███▉ | 7.94G/9.94G [00:30<00:07, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  80%|████ | 7.97G/9.94G [00:30<00:07, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  80%|████ | 8.00G/9.94G [00:30<00:07, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  81%|████ | 8.03G/9.94G [00:31<00:07, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  81%|████ | 8.06G/9.94G [00:31<00:07, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  81%|████ | 8.10G/9.94G [00:31<00:07, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  82%|████ | 8.13G/9.94G [00:31<00:07, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  82%|████ | 8.16G/9.94G [00:31<00:07, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  82%|████ | 8.19G/9.94G [00:31<00:06, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  83%|████▏| 8.22G/9.94G [00:31<00:06, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  83%|████▏| 8.25G/9.94G [00:31<00:06, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  83%|████▏| 8.28G/9.94G [00:32<00:06, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  84%|████▏| 8.32G/9.94G [00:32<00:06, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  84%|████▏| 8.35G/9.94G [00:32<00:06, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  84%|████▏| 8.38G/9.94G [00:32<00:06, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  85%|████▏| 8.41G/9.94G [00:32<00:06, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  85%|████▏| 8.44G/9.94G [00:32<00:05, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  85%|████▎| 8.47G/9.94G [00:32<00:05, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  86%|████▎| 8.50G/9.94G [00:32<00:05, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  86%|████▎| 8.54G/9.94G [00:33<00:05, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  86%|████▎| 8.57G/9.94G [00:33<00:05, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  86%|████▎| 8.60G/9.94G [00:33<00:05, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  87%|████▎| 8.63G/9.94G [00:33<00:05, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  87%|████▎| 8.66G/9.94G [00:33<00:05, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  87%|████▎| 8.69G/9.94G [00:33<00:04, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  88%|████▍| 8.72G/9.94G [00:33<00:04, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  88%|████▍| 8.76G/9.94G [00:33<00:04, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  88%|████▍| 8.79G/9.94G [00:34<00:04, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  89%|████▍| 8.82G/9.94G [00:34<00:04, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  89%|████▍| 8.85G/9.94G [00:34<00:04, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  89%|████▍| 8.88G/9.94G [00:34<00:04, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  90%|████▍| 8.91G/9.94G [00:34<00:04, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  90%|████▍| 8.94G/9.94G [00:34<00:03, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  90%|████▌| 8.98G/9.94G [00:34<00:03, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  91%|████▌| 9.01G/9.94G [00:34<00:03, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  91%|████▌| 9.04G/9.94G [00:34<00:03, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  91%|████▌| 9.07G/9.94G [00:35<00:03, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  92%|████▌| 9.10G/9.94G [00:35<00:03, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  92%|████▌| 9.13G/9.94G [00:35<00:03, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  92%|████▌| 9.16G/9.94G [00:35<00:03, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  92%|████▌| 9.20G/9.94G [00:35<00:02, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  93%|████▋| 9.23G/9.94G [00:35<00:02, 253MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  93%|████▋| 9.26G/9.94G [00:35<00:02, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  93%|████▋| 9.29G/9.94G [00:35<00:02, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  94%|████▋| 9.32G/9.94G [00:36<00:02, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  94%|████▋| 9.35G/9.94G [00:36<00:02, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  94%|████▋| 9.38G/9.94G [00:36<00:02, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  95%|████▋| 9.42G/9.94G [00:36<00:02, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  95%|████▊| 9.45G/9.94G [00:36<00:01, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  95%|████▊| 9.48G/9.94G [00:36<00:02, 177MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  96%|████▊| 9.52G/9.94G [00:37<00:01, 219MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  96%|████▊| 9.56G/9.94G [00:37<00:01, 260MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  97%|████▊| 9.60G/9.94G [00:37<00:01, 275MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  97%|████▊| 9.64G/9.94G [00:37<00:01, 269MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  97%|████▊| 9.67G/9.94G [00:37<00:01, 265MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  98%|████▉| 9.70G/9.94G [00:37<00:00, 261MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  98%|████▉| 9.73G/9.94G [00:37<00:00, 259MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  98%|████▉| 9.76G/9.94G [00:37<00:00, 257MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  98%|████▉| 9.79G/9.94G [00:38<00:00, 257MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  99%|████▉| 9.83G/9.94G [00:38<00:00, 256MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  99%|████▉| 9.86G/9.94G [00:38<00:00, 255MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  99%|████▉| 9.89G/9.94G [00:38<00:00, 255MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors: 100%|█████| 9.94G/9.94G [00:38<00:00, 258MB/s]\u001b[A\n",
      "Downloading shards:  50%|████████████▌            | 1/2 [00:38<00:38, 38.61s/it]\n",
      "model-00002-of-00002.safetensors:   0%|             | 0.00/4.54G [00:00<?, ?B/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   0%|     | 21.0M/4.54G [00:00<00:27, 162MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   1%|     | 41.9M/4.54G [00:00<00:24, 182MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   2%|     | 83.9M/4.54G [00:00<00:16, 262MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   3%|▏     | 115M/4.54G [00:00<00:15, 280MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   3%|▏     | 147M/4.54G [00:00<00:16, 270MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   4%|▏     | 178M/4.54G [00:00<00:16, 264MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   5%|▎     | 210M/4.54G [00:00<00:16, 260MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   5%|▎     | 241M/4.54G [00:00<00:16, 258MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   6%|▎     | 273M/4.54G [00:01<00:16, 257MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   7%|▍     | 304M/4.54G [00:01<00:16, 256MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   7%|▍     | 336M/4.54G [00:01<00:16, 256MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   8%|▍     | 367M/4.54G [00:01<00:16, 255MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   9%|▌     | 398M/4.54G [00:01<00:16, 255MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   9%|▌     | 430M/4.54G [00:01<00:16, 254MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  10%|▌     | 461M/4.54G [00:01<00:16, 254MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  11%|▋     | 493M/4.54G [00:01<00:15, 254MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  12%|▋     | 524M/4.54G [00:02<00:15, 254MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  12%|▋     | 556M/4.54G [00:02<00:15, 254MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  13%|▊     | 587M/4.54G [00:02<00:15, 254MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  14%|▊     | 619M/4.54G [00:02<00:15, 254MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  14%|▊     | 650M/4.54G [00:02<00:15, 254MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  15%|▉     | 682M/4.54G [00:02<00:15, 254MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  16%|▉     | 713M/4.54G [00:02<00:15, 254MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  16%|▉     | 744M/4.54G [00:02<00:14, 254MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  17%|█     | 776M/4.54G [00:03<00:14, 254MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  18%|█     | 807M/4.54G [00:03<00:14, 254MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  18%|█     | 839M/4.54G [00:03<00:14, 254MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  19%|█▏    | 870M/4.54G [00:03<00:14, 254MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  20%|█▏    | 902M/4.54G [00:03<00:14, 254MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  21%|█▏    | 933M/4.54G [00:03<00:14, 254MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  21%|█▎    | 965M/4.54G [00:03<00:14, 254MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  22%|█▎    | 996M/4.54G [00:03<00:13, 254MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  23%|█▏   | 1.03G/4.54G [00:04<00:13, 254MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  23%|█▏   | 1.06G/4.54G [00:04<00:13, 254MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  24%|█▏   | 1.09G/4.54G [00:04<00:13, 254MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  25%|█▏   | 1.12G/4.54G [00:04<00:13, 254MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  25%|█▎   | 1.15G/4.54G [00:04<00:13, 254MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  26%|█▎   | 1.18G/4.54G [00:04<00:13, 254MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  27%|█▎   | 1.22G/4.54G [00:04<00:13, 254MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  27%|█▎   | 1.25G/4.54G [00:04<00:12, 253MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  28%|█▍   | 1.28G/4.54G [00:05<00:12, 254MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  29%|█▍   | 1.31G/4.54G [00:05<00:12, 254MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  30%|█▍   | 1.34G/4.54G [00:05<00:12, 254MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  30%|█▌   | 1.37G/4.54G [00:05<00:12, 254MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  31%|█▌   | 1.41G/4.54G [00:05<00:12, 253MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  32%|█▌   | 1.44G/4.54G [00:05<00:12, 254MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  32%|█▌   | 1.47G/4.54G [00:05<00:12, 254MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  33%|█▋   | 1.50G/4.54G [00:05<00:11, 254MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  34%|█▋   | 1.53G/4.54G [00:06<00:11, 254MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  34%|█▋   | 1.56G/4.54G [00:06<00:11, 254MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  35%|█▊   | 1.59G/4.54G [00:06<00:11, 254MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  36%|█▊   | 1.63G/4.54G [00:06<00:11, 254MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  36%|█▊   | 1.66G/4.54G [00:06<00:11, 254MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  37%|█▊   | 1.69G/4.54G [00:06<00:11, 254MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  38%|█▉   | 1.72G/4.54G [00:06<00:11, 253MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  39%|█▉   | 1.75G/4.54G [00:06<00:10, 254MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  39%|█▉   | 1.78G/4.54G [00:07<00:10, 254MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  40%|█▉   | 1.81G/4.54G [00:07<00:10, 254MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  41%|██   | 1.85G/4.54G [00:07<00:10, 254MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  41%|██   | 1.88G/4.54G [00:07<00:10, 254MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  42%|██   | 1.91G/4.54G [00:07<00:10, 254MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  43%|██▏  | 1.94G/4.54G [00:07<00:10, 254MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  43%|██▏  | 1.97G/4.54G [00:07<00:10, 254MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  44%|██▏  | 2.00G/4.54G [00:07<00:09, 254MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  45%|██▏  | 2.03G/4.54G [00:08<00:09, 254MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  45%|██▎  | 2.07G/4.54G [00:08<00:09, 254MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  46%|██▎  | 2.10G/4.54G [00:08<00:09, 253MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  47%|██▎  | 2.13G/4.54G [00:08<00:09, 254MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  48%|██▍  | 2.16G/4.54G [00:08<00:09, 254MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  48%|██▍  | 2.19G/4.54G [00:08<00:09, 254MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  49%|██▍  | 2.22G/4.54G [00:08<00:09, 254MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  50%|██▍  | 2.25G/4.54G [00:08<00:08, 254MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  50%|██▌  | 2.29G/4.54G [00:09<00:08, 254MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  51%|██▌  | 2.32G/4.54G [00:09<00:08, 254MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  52%|██▌  | 2.35G/4.54G [00:09<00:08, 254MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  52%|██▌  | 2.38G/4.54G [00:09<00:08, 254MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  53%|██▋  | 2.41G/4.54G [00:09<00:08, 254MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  54%|██▋  | 2.44G/4.54G [00:09<00:08, 254MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  55%|██▋  | 2.47G/4.54G [00:09<00:08, 254MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  55%|██▊  | 2.51G/4.54G [00:09<00:08, 254MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  56%|██▊  | 2.54G/4.54G [00:09<00:07, 254MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  57%|██▊  | 2.57G/4.54G [00:10<00:07, 254MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  57%|██▊  | 2.60G/4.54G [00:10<00:07, 254MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  58%|██▉  | 2.63G/4.54G [00:10<00:08, 222MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  59%|██▉  | 2.67G/4.54G [00:10<00:07, 253MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  60%|██▉  | 2.71G/4.54G [00:10<00:10, 176MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  60%|███  | 2.74G/4.54G [00:10<00:09, 197MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  61%|███  | 2.78G/4.54G [00:11<00:07, 237MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  62%|███  | 2.82G/4.54G [00:11<00:06, 273MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  63%|███▏ | 2.86G/4.54G [00:11<00:05, 295MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  64%|███▏ | 2.90G/4.54G [00:11<00:05, 279MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  65%|███▏ | 2.94G/4.54G [00:11<00:05, 273MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  65%|███▎ | 2.97G/4.54G [00:11<00:05, 268MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  66%|███▎ | 3.00G/4.54G [00:11<00:05, 263MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  67%|███▎ | 3.03G/4.54G [00:11<00:05, 261MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  67%|███▎ | 3.06G/4.54G [00:12<00:05, 259MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  68%|███▍ | 3.09G/4.54G [00:12<00:05, 257MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  69%|███▍ | 3.12G/4.54G [00:12<00:05, 257MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  70%|███▍ | 3.16G/4.54G [00:12<00:05, 255MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  70%|███▌ | 3.19G/4.54G [00:12<00:05, 255MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  71%|███▌ | 3.22G/4.54G [00:12<00:05, 255MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  72%|███▌ | 3.25G/4.54G [00:12<00:05, 254MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  72%|███▌ | 3.28G/4.54G [00:12<00:04, 254MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  73%|███▋ | 3.31G/4.54G [00:13<00:04, 254MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  74%|███▋ | 3.34G/4.54G [00:13<00:04, 254MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  74%|███▋ | 3.38G/4.54G [00:13<00:04, 253MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  75%|███▊ | 3.41G/4.54G [00:13<00:04, 254MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  76%|███▊ | 3.44G/4.54G [00:13<00:04, 254MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  76%|███▊ | 3.47G/4.54G [00:13<00:04, 254MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  77%|███▊ | 3.50G/4.54G [00:13<00:04, 254MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  78%|███▉ | 3.53G/4.54G [00:13<00:03, 254MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  79%|███▉ | 3.57G/4.54G [00:14<00:03, 254MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  79%|███▉ | 3.60G/4.54G [00:14<00:03, 254MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  80%|███▉ | 3.63G/4.54G [00:14<00:03, 254MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  81%|████ | 3.66G/4.54G [00:14<00:03, 254MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  81%|████ | 3.69G/4.54G [00:14<00:03, 254MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  82%|████ | 3.72G/4.54G [00:14<00:03, 254MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  83%|████▏| 3.75G/4.54G [00:14<00:03, 254MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  83%|████▏| 3.79G/4.54G [00:14<00:02, 254MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  84%|████▏| 3.82G/4.54G [00:15<00:02, 254MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  85%|████▏| 3.85G/4.54G [00:15<00:02, 254MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  85%|████▎| 3.88G/4.54G [00:15<00:02, 254MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  86%|████▎| 3.91G/4.54G [00:15<00:02, 254MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  87%|████▎| 3.94G/4.54G [00:15<00:02, 254MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  88%|████▍| 3.97G/4.54G [00:15<00:02, 254MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  88%|████▍| 4.01G/4.54G [00:15<00:02, 254MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  89%|████▍| 4.04G/4.54G [00:15<00:01, 253MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  90%|████▍| 4.07G/4.54G [00:16<00:01, 254MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  90%|████▌| 4.10G/4.54G [00:16<00:01, 254MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  91%|████▌| 4.13G/4.54G [00:16<00:01, 254MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  92%|████▌| 4.16G/4.54G [00:16<00:01, 254MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  92%|████▌| 4.19G/4.54G [00:16<00:01, 254MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  93%|████▋| 4.23G/4.54G [00:16<00:01, 254MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  94%|████▋| 4.26G/4.54G [00:16<00:01, 254MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  94%|████▋| 4.29G/4.54G [00:16<00:00, 254MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  95%|████▊| 4.32G/4.54G [00:17<00:00, 254MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  96%|████▊| 4.35G/4.54G [00:17<00:00, 254MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  97%|████▊| 4.38G/4.54G [00:17<00:00, 254MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  97%|████▊| 4.41G/4.54G [00:17<00:00, 254MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  98%|████▉| 4.45G/4.54G [00:17<00:00, 254MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  99%|████▉| 4.48G/4.54G [00:17<00:00, 147MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors: 100%|█████| 4.54G/4.54G [00:18<00:00, 250MB/s]\u001b[A\n",
      "Downloading shards: 100%|█████████████████████████| 2/2 [00:56<00:00, 28.39s/it]\n",
      "[INFO|modeling_utils.py:1400] 2024-03-13 20:38:41,218 >> Instantiating MistralForCausalLM model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:845] 2024-03-13 20:38:41,219 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:02<00:00,  1.40s/it]\n",
      "[INFO|modeling_utils.py:3992] 2024-03-13 20:38:44,353 >> All model checkpoint weights were used when initializing MistralForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4000] 2024-03-13 20:38:44,353 >> All the weights of MistralForCausalLM were initialized from the model checkpoint at mistralai/Mistral-7B-Instruct-v0.1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MistralForCausalLM for predictions without further training.\n",
      "generation_config.json: 100%|██████████████████| 116/116 [00:00<00:00, 1.17MB/s]\n",
      "[INFO|configuration_utils.py:800] 2024-03-13 20:38:44,402 >> loading configuration file generation_config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.1/snapshots/73068f3702d050a2fd5aa2ca1e612e5036429398/generation_config.json\n",
      "[INFO|configuration_utils.py:845] 2024-03-13 20:38:44,402 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      "\n",
      "03/13/2024 20:38:44 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.\n",
      "03/13/2024 20:38:44 - INFO - llmtuner.model.adapter - Fine-tuning method: LoRA\n",
      "03/13/2024 20:38:44 - INFO - llmtuner.model.loader - trainable params: 3407872 || all params: 7245139968 || trainable%: 0.0470\n",
      "[INFO|trainer.py:601] 2024-03-13 20:38:44,700 >> Using auto half precision backend\n",
      "[INFO|trainer.py:1812] 2024-03-13 20:38:44,856 >> ***** Running training *****\n",
      "[INFO|trainer.py:1813] 2024-03-13 20:38:44,856 >>   Num examples = 2,294\n",
      "[INFO|trainer.py:1814] 2024-03-13 20:38:44,856 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1815] 2024-03-13 20:38:44,856 >>   Instantaneous batch size per device = 16\n",
      "[INFO|trainer.py:1818] 2024-03-13 20:38:44,856 >>   Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "[INFO|trainer.py:1819] 2024-03-13 20:38:44,856 >>   Gradient Accumulation steps = 4\n",
      "[INFO|trainer.py:1820] 2024-03-13 20:38:44,856 >>   Total optimization steps = 36\n",
      "[INFO|trainer.py:1821] 2024-03-13 20:38:44,858 >>   Number of trainable parameters = 3,407,872\n",
      "03/13/2024 20:39:08 - INFO - llmtuner.extras.callbacks - {'loss': 0.8597, 'learning_rate': 4.7658e-05, 'epoch': 0.14}\n",
      "{'loss': 0.8597, 'grad_norm': 1.8740241527557373, 'learning_rate': 4.765769467591625e-05, 'epoch': 0.14}\n",
      "03/13/2024 20:39:32 - INFO - llmtuner.extras.callbacks - {'loss': 0.6283, 'learning_rate': 4.1070e-05, 'epoch': 0.28}\n",
      "{'loss': 0.6283, 'grad_norm': 1.5111083984375, 'learning_rate': 4.1069690242163484e-05, 'epoch': 0.28}\n",
      "03/13/2024 20:39:56 - INFO - llmtuner.extras.callbacks - {'loss': 0.5246, 'learning_rate': 3.1470e-05, 'epoch': 0.42}\n",
      "{'loss': 0.5246, 'grad_norm': 1.2706975936889648, 'learning_rate': 3.147047612756302e-05, 'epoch': 0.42}\n",
      "03/13/2024 20:40:20 - INFO - llmtuner.extras.callbacks - {'loss': 0.4449, 'learning_rate': 2.0659e-05, 'epoch': 0.56}\n",
      "{'loss': 0.4449, 'grad_norm': 1.0707210302352905, 'learning_rate': 2.0658795558326743e-05, 'epoch': 0.56}\n",
      "03/13/2024 20:40:44 - INFO - llmtuner.extras.callbacks - {'loss': 0.4050, 'learning_rate': 1.0661e-05, 'epoch': 0.69}\n",
      "{'loss': 0.405, 'grad_norm': 1.0247399806976318, 'learning_rate': 1.0660589091223855e-05, 'epoch': 0.69}\n",
      "03/13/2024 20:41:10 - INFO - llmtuner.extras.callbacks - {'loss': 0.4021, 'learning_rate': 3.3494e-06, 'epoch': 0.83}\n",
      "{'loss': 0.4021, 'grad_norm': 1.002211332321167, 'learning_rate': 3.3493649053890326e-06, 'epoch': 0.83}\n",
      "03/13/2024 20:41:34 - INFO - llmtuner.extras.callbacks - {'loss': 0.3795, 'learning_rate': 9.5133e-08, 'epoch': 0.97}\n",
      "{'loss': 0.3795, 'grad_norm': 1.0953750610351562, 'learning_rate': 9.513254770636137e-08, 'epoch': 0.97}\n",
      "[INFO|trainer.py:2067] 2024-03-13 20:41:39,278 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "03/13/2024 20:41:39 - INFO - llmtuner.extras.callbacks - {'loss': 0.0000, 'learning_rate': 0.0000e+00, 'epoch': 1.00}\n",
      "{'train_runtime': 174.421, 'train_samples_per_second': 13.152, 'train_steps_per_second': 0.206, 'train_loss': 0.5156687051057816, 'epoch': 1.0}\n",
      "[INFO|trainer.py:3067] 2024-03-13 20:41:39,287 >> Saving model checkpoint to saves/Mistral-7B-Chat/lora/train_2024-03-13-19-46-19\n",
      "[INFO|configuration_utils.py:728] 2024-03-13 20:41:39,367 >> loading configuration file config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.1/snapshots/73068f3702d050a2fd5aa2ca1e612e5036429398/config.json\n",
      "[INFO|configuration_utils.py:791] 2024-03-13 20:41:39,367 >> Model config MistralConfig {\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 4096,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.38.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2459] 2024-03-13 20:41:39,513 >> tokenizer config file saved in saves/Mistral-7B-Chat/lora/train_2024-03-13-19-46-19/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2468] 2024-03-13 20:41:39,522 >> Special tokens file saved in saves/Mistral-7B-Chat/lora/train_2024-03-13-19-46-19/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     0.5157\n",
      "  train_runtime            = 0:02:54.42\n",
      "  train_samples_per_second =     13.152\n",
      "  train_steps_per_second   =      0.206\n",
      "[INFO|trainer.py:3376] 2024-03-13 20:41:39,597 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3378] 2024-03-13 20:41:39,597 >>   Num examples = 121\n",
      "[INFO|trainer.py:3381] 2024-03-13 20:41:39,597 >>   Batch size = 16\n",
      "03/13/2024 20:41:42 - INFO - llmtuner.extras.callbacks - {'loss': 0.0000, 'learning_rate': 0.0000e+00, 'epoch': 1.00}\n",
      "{'eval_loss': 0.33579933643341064, 'eval_runtime': 2.825, 'eval_samples_per_second': 42.832, 'eval_steps_per_second': 2.832, 'epoch': 1.0}\n",
      "***** eval metrics *****\n",
      "  epoch                   =        1.0\n",
      "  eval_loss               =     0.3358\n",
      "  eval_runtime            = 0:00:02.82\n",
      "  eval_samples_per_second =     42.832\n",
      "  eval_steps_per_second   =      2.832\n",
      "[INFO|modelcard.py:450] 2024-03-13 20:41:42,448 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/gradio/queueing.py\", line 407, in call_prediction\n",
      "    output = await route_utils.call_process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/gradio/route_utils.py\", line 226, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/gradio/blocks.py\", line 1550, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/gradio/blocks.py\", line 1199, in call_function\n",
      "    prediction = await utils.async_iteration(iterator)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/gradio/utils.py\", line 519, in async_iteration\n",
      "    return await iterator.__anext__()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/gradio/utils.py\", line 512, in __anext__\n",
      "    return await anyio.to_thread.run_sync(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 2144, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 851, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/gradio/utils.py\", line 495, in run_sync_iterator_async\n",
      "    return next(iterator)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/gradio/utils.py\", line 649, in gen_wrapper\n",
      "    yield from f(*args, **kwargs)\n",
      "  File \"/home/ubuntu/quamer/Fine-Tuning-Mistral-7B-Using-Llama-Factory/LLaMA-Factory/src/llmtuner/webui/runner.py\", line 270, in preview_eval\n",
      "    yield from self._preview(data, do_train=False)\n",
      "  File \"/home/ubuntu/quamer/Fine-Tuning-Mistral-7B-Using-Llama-Factory/LLaMA-Factory/src/llmtuner/webui/runner.py\", line 245, in _preview\n",
      "    error = self._initialize(data, do_train, from_preview=True)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/Fine-Tuning-Mistral-7B-Using-Llama-Factory/LLaMA-Factory/src/llmtuner/webui/runner.py\", line 55, in _initialize\n",
      "    stage = TRAINING_STAGES[get(\"train.training_stage\")]\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/Fine-Tuning-Mistral-7B-Using-Llama-Factory/LLaMA-Factory/src/llmtuner/webui/runner.py\", line 52, in <lambda>\n",
      "    get = lambda name: data[self.manager.get_elem_by_name(name)]\n",
      "                       ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyError: dropdown\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/gradio/queueing.py\", line 407, in call_prediction\n",
      "    output = await route_utils.call_process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/gradio/route_utils.py\", line 226, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/gradio/blocks.py\", line 1550, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/gradio/blocks.py\", line 1199, in call_function\n",
      "    prediction = await utils.async_iteration(iterator)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/gradio/utils.py\", line 519, in async_iteration\n",
      "    return await iterator.__anext__()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/gradio/utils.py\", line 512, in __anext__\n",
      "    return await anyio.to_thread.run_sync(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 2144, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 851, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/gradio/utils.py\", line 495, in run_sync_iterator_async\n",
      "    return next(iterator)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/gradio/utils.py\", line 649, in gen_wrapper\n",
      "    yield from f(*args, **kwargs)\n",
      "  File \"/home/ubuntu/quamer/Fine-Tuning-Mistral-7B-Using-Llama-Factory/LLaMA-Factory/src/llmtuner/webui/runner.py\", line 270, in preview_eval\n",
      "    yield from self._preview(data, do_train=False)\n",
      "  File \"/home/ubuntu/quamer/Fine-Tuning-Mistral-7B-Using-Llama-Factory/LLaMA-Factory/src/llmtuner/webui/runner.py\", line 245, in _preview\n",
      "    error = self._initialize(data, do_train, from_preview=True)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/Fine-Tuning-Mistral-7B-Using-Llama-Factory/LLaMA-Factory/src/llmtuner/webui/runner.py\", line 55, in _initialize\n",
      "    stage = TRAINING_STAGES[get(\"train.training_stage\")]\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/Fine-Tuning-Mistral-7B-Using-Llama-Factory/LLaMA-Factory/src/llmtuner/webui/runner.py\", line 52, in <lambda>\n",
      "    get = lambda name: data[self.manager.get_elem_by_name(name)]\n",
      "                       ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyError: dropdown\n",
      "[INFO|training_args.py:1902] 2024-03-13 20:42:44,112 >> PyTorch: setting up devices\n",
      "[INFO|training_args.py:1611] 2024-03-13 20:42:44,112 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "03/13/2024 20:42:44 - WARNING - llmtuner.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.\n",
      "Exception in thread Thread-17 (run_exp):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/threading.py\", line 1073, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/threading.py\", line 1010, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ubuntu/quamer/Fine-Tuning-Mistral-7B-Using-Llama-Factory/LLaMA-Factory/src/llmtuner/train/tuner.py\", line 26, in run_exp\n",
      "    model_args, data_args, training_args, finetuning_args, generating_args = get_train_args(args)\n",
      "                                                                             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/Fine-Tuning-Mistral-7B-Using-Llama-Factory/LLaMA-Factory/src/llmtuner/hparams/parser.py\", line 205, in get_train_args\n",
      "    raise ValueError(\"Output directory already exists and is not empty. Please set `overwrite_output_dir`.\")\n",
      "ValueError: Output directory already exists and is not empty. Please set `overwrite_output_dir`.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/gradio/queueing.py\", line 407, in call_prediction\n",
      "    output = await route_utils.call_process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/gradio/route_utils.py\", line 226, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/gradio/blocks.py\", line 1550, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/gradio/blocks.py\", line 1199, in call_function\n",
      "    prediction = await utils.async_iteration(iterator)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/gradio/utils.py\", line 519, in async_iteration\n",
      "    return await iterator.__anext__()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/gradio/utils.py\", line 512, in __anext__\n",
      "    return await anyio.to_thread.run_sync(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 2144, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 851, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/gradio/utils.py\", line 495, in run_sync_iterator_async\n",
      "    return next(iterator)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/miniconda3/lib/python3.12/site-packages/gradio/utils.py\", line 649, in gen_wrapper\n",
      "    yield from f(*args, **kwargs)\n",
      "  File \"/home/ubuntu/quamer/Fine-Tuning-Mistral-7B-Using-Llama-Factory/LLaMA-Factory/src/llmtuner/webui/runner.py\", line 270, in preview_eval\n",
      "    yield from self._preview(data, do_train=False)\n",
      "  File \"/home/ubuntu/quamer/Fine-Tuning-Mistral-7B-Using-Llama-Factory/LLaMA-Factory/src/llmtuner/webui/runner.py\", line 245, in _preview\n",
      "    error = self._initialize(data, do_train, from_preview=True)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/Fine-Tuning-Mistral-7B-Using-Llama-Factory/LLaMA-Factory/src/llmtuner/webui/runner.py\", line 55, in _initialize\n",
      "    stage = TRAINING_STAGES[get(\"train.training_stage\")]\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/quamer/Fine-Tuning-Mistral-7B-Using-Llama-Factory/LLaMA-Factory/src/llmtuner/webui/runner.py\", line 52, in <lambda>\n",
      "    get = lambda name: data[self.manager.get_elem_by_name(name)]\n",
      "                       ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyError: dropdown\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 python src/train_web.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_VISIBLE_DEVICES=0 python src/train_bash.py \\\n",
    "    --stage sft \\\n",
    "    --do_train True \\\n",
    "    --model_name_or_path mistralai/Mistral-7B-Instruct-v0.2 \\\n",
    "    --finetuning_type lora \\\n",
    "    --quantization_bit 4 \\\n",
    "    --template mistral-instruction-v02 \\\n",
    "    --flash_attn True \\\n",
    "    --dataset_dir data \\\n",
    "    --dataset dockerNLCommands \\\n",
    "    --cutoff_len 512 \\\n",
    "    --learning_rate 0.0002 \\\n",
    "    --num_train_epochs 10.0 \\\n",
    "    --max_samples 10000 \\\n",
    "    --per_device_train_batch_size 64 \\\n",
    "    --gradient_accumulation_steps 8 \\\n",
    "    --lr_scheduler_type cosine \\\n",
    "    --max_grad_norm 0.3 \\\n",
    "    --logging_steps 5 \\\n",
    "    --save_steps 50 \\\n",
    "    --warmup_steps 0 \\\n",
    "    --optim adamw_torch \\\n",
    "    --output_dir saves/Custom/lora/train_2024-03-13-10-46-24 \\\n",
    "    --bf16 True \\\n",
    "    --lora_rank 8 \\\n",
    "    --lora_alpha 16 \\\n",
    "    --lora_dropout 0.1 \\\n",
    "    --lora_target q_proj, v_proj \\\n",
    "    --val_size 0.1 \\\n",
    "    --evaluation_strategy steps \\\n",
    "    --eval_steps 50 \\\n",
    "    --per_device_eval_batch_size 64 \\\n",
    "    --load_best_model_at_end True \\\n",
    "    --plot_loss True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we launch the web-version of llama-factory, we can access the gradio interface by visiting the URL shown in the terminal. We're provided with two types of URL, one local URL and another one is an external/public URL. We can use any of them to access the llama-factory gradio interface. If you're using the google colab, you can only use the public URL, whereas if you're a Lambda Cloud system, you can use both the local and public URL.\n",
    "\n",
    "Let's now look at the llama-factory web interface.\n",
    "\n",
    "<img src=\"assets/gradio-interface.png\" alt=\"share=True\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woah! That's a lot of options!\n",
    "\n",
    "We can see that we have a lot of options to choose from. We can select the model, the dataset, the hyperparameters, and the training options, etc. Everything is customizable. We can also see the training logs and the training progress in the web interface. This is a very powerful tool and can be used to train models on the cloud without any hassle and worry about the coding part.\n",
    "\n",
    "Let's now first try to understand all the available configurations provided by the llama-factory.\n",
    "\n",
    "As of now, Llama-factory supports 3 different language in their UI. They are: en (English), ru (Russian), and zh (Chinese). By default, the language is set to English. We can change the language by selecting the language from the dropdown menu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Llama-factory provides a lot of models to choose from. We can select the model from the dropdown menu. This is what makes llama-factory so powerful. All the popular models are available to choose from. In the backend, llama-factory uses the Hugging Face model hub to fetch the models.\n",
    "\n",
    "<img src=\"assets/model_selection.png\" alt=\"share=True\">\n",
    "\n",
    "Though llama-factory provides a lot of models, it still gives us the flexibility to use custom models. We can use the custom model by providing the model path from the Hugging Face model hub. In order to use the custom model, we need to select the \"Custom\" option from the dropdown menu and provide the model path in the Model Path input field.\n",
    "\n",
    "\n",
    "In this blog, we'll be using a custom model. We'll be using instruction finetuned mistral model. Before we proceed, let's first understand what is mistral-instruction-v02 model. Mistral-instruction-v02 is a model that is instruction-fine-tuned version of the mistral-7b model. It is a large language model with 7.3 billion parameters. Even being a comparatively smaller model, it has outperformed larger models like Llama 2 (13 billion parameters) on various benchmarks. It uses grouped-query and sliding window attention to tackle sequences of arbitrary length efficiently. [Grouped-query attention](https://klu.ai/glossary/grouped-query-attention) is a technique that speeds up attention by grouping query vectors. Each group shares a single key and value vector, reducing computations compared to standard attention. [Sliding window attention](https://klu.ai/glossary/sliding-window-attention) handles long sequences by focusing on smaller chunks (windows) at a time. The window slides along the sequence, processing each section efficiently. Combining these two key-techinques, Mistral-7b offers a good balance between speed and performance. It excels in tasks like reasoning, math, and code generation. If you want to know more about the mistral-7b model, you read the paper by visiting [this link](https://arxiv.org/abs/2310.06825).\n",
    "\n",
    "Now let's get back to the llama-factory. Though llama-factory does provide the mistral-insturction model, under the name Mistral-7B-Chat, but that's v.01. We'll be using the v.02 model. mistral-instruction-v02 is an improved version of mistral-instruction-v01. You can find the model from the Hugging Face model hub by visiting [this link](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2). We'll be using this model to train our model. You can copy the model path from the model hub and paste it in the Model Path input field.\n",
    "\n",
    "<img src=\"assets/model config.png\" alt=\"share=True\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tuning Configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have selected the model, we can now move to the next configuration, i.e., Fine Tuning Configurations. Llama-factory provides us with 3 different options to fine-tune the model. They are: full, freeze, and lora. By default, the fine-tuning option is set to lora. We can change the fine-tuning option by selecting the option from the dropdown menu. Let's understand what these options mean. When we select the full option, the entire model is fine-tuned. This means that all the layers of the model are fine-tuned. When we select the freeze option, the model is not fine-tuned at all. This means that the model is used as it is for evaluation and inference purposes. The default option, i.e., lora, stands for Low Rank Adaptation. When we select the lora option, the model is fine-tuned using low rank adaptation technique. Low rank adaptation is a technique that fine-tunes the model using low-rank matrices. This technique significantly reduces training time, memory usage, and computational power needed. It's like tweaking a small dial on a large machine for precise adjustments. This is a very powerful technique and is a go-to option when we want to fine-tune large models. It essentially freezes the model's parameters and introduces a low-rank matrix to different layers of the model, which is then fine-tuned. If you want to know more about the low rank adaptation technique, you can read the paper by visiting [this link](https://arxiv.org/abs/2106.09685)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/finetuning_method.png\" alt=\"share=True\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have selected the fine-tuning option, we now get the option to pass something called Adaptors. Adaptors are nothing but model checkpoint. We can pass the model checkpoint by providing the model path in the Adaptors input field. This is an optional field. If you're working on llama-factory for the first time, you will not have any model checkpoint that has been adapted to a specific task. You can leave this field empty. Once you're trained a model using llama-factory, you will then get options to pass the adaptors. It's like a checkpoint that you can use to either resume the training or to fine-tune the model further or to use the model for inference purposes. We will revisit this field once we have trained a model using llama-factory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Configurations\n",
    "\n",
    "<img src=\"assets/advanced_config.png\" alt=\"share=True\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know how to select the model and the fine-tuning options, we can now move to the next configuration, i.e., Advanced Configurations. Llama-factory provides us with a lot of advanced configurations that help us to fine-tune the model efficiently. There are mainly 4 different options available in the advanced configurations. They are: Quantization Bit, Prompt Template, RoPE Scaling, and Boosters. Let's understand what these options mean.\n",
    "\n",
    "\n",
    "Quantization Bit: Quantization is a technique that reduces the precision of the model's parameters. This reduces the memory usage and computational power needed. Llama-factory provides us with the option to quantize the model's parameters. We can select the quantization bit from the dropdown menu. By default, the quantization bit is set to 4. The available options are 4 and 8. Llama-factory uses the QLoRA technique for quantization. It basically is quantization and low-rank adaptation. This method will allow us to fine-tune massive models on a single GPU. If you want to know more about the QLoRA technique, you can read the paper by visiting [this link](https://arxiv.org/abs/2305.14314)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/template path.png\" alt=\"share=True\">\n",
    "<img src=\"assets/template.png\" alt=\"share=True\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt Template: We also need to provide the prompt template. You can get the information about the prompt template on respective model's hugingface model hub page. For example, the prompt template for mistral-instruction-v02 model is [INST] {{content}} [/INST]. So we first need to create a prompt template, and for that, we need to modify some code in the src/llmtuner/data/template.py file (look in the image below). In this file we nned to register the template. Here we'll see that llama-factory provides us with many pre-registered templates. We can use any of them based on our requirement. If we want to use a custom template, we can do that by adding the following code in the template.py file.\n",
    "\n",
    "```\n",
    "_register_template(\n",
    "    name=\"mistral-instruction-v02\",\n",
    "    format_user=StringFormatter(slots=[\"[INST] {{content}} [/INST]\"]),\n",
    "    format_system=StringFormatter(slots=[{\"bos_token\"}, \"{{content}}\"]),\n",
    "    force_system=True,\n",
    ")\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RoPE Scaling: RoPE stands for Rotary Position Embeddings. RoPE is used in LLMs to understand the relative position of words within a sequence. What RoPE Scaling does is that it modifies the RoPE calculations to improve the model's ability to handle longer sequences. It does this by tweaking the base valu used in the RoPE calculations. This value controls the rate at which the sine and cosine functions oscillate, which basically at the end affects the text-embeddings. Increasing the base value can spread out the embeddings, making them more distinct for longer sequences. While decreasing it can introduce periodicity, allowing the model to handle longer sequences that wrap around this cycle. Llama-factory provides us with the option to scale the RoPE. We can select the RoPE scaling from given options. By default, the RoPE scaling is set to None. The available options are None, Linear, and Dynamic. Linear RoPE scaling involves scaling the wavelength linearly by a factor of intended maximum sequence length to the model's original maximum sequence length. This adjustment ensures that the entire period window is fully utilized by all token positions when the wavelength is less than the context length. Dynamic RoPE scaling adjusts the base with a coefficient that increases with the length of inference. It is particularly useful for adapting RoPE to longer contexts without fine-tuning. We can select the RoPE scaling based on our requirement. If you want to know more about the RoPE scaling, you can read the paper by visiting [this link](https://arxiv.org/abs/2310.05209) or look at this [blog 1](https://www.hopsworks.ai/dictionary/rope-scalingg) and [blog 2](https://blog.eleuther.ai/yarn/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to train a model using Llama-Factory?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
